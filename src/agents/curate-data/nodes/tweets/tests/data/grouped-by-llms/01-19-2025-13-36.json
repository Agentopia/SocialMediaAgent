[
  {
    "explanation": "MLX library usage: tweets discussing MLX or tools built around it for model training or deployment.",
    "tweetIndicies": [0, 50, 72, 88]
  },
  {
    "explanation": "Philosophical musings on AGI and singularity: tweets discussing AI surpassing humans or rewriting civilization's software stack.",
    "tweetIndicies": [1, 4, 17, 84]
  },
  {
    "explanation": "AI-based video & creative generation: tweets about Argil, Sora, Ray2, Veo2, runwayml, 3D/4D visuals, or fully AI-generated video content.",
    "tweetIndicies": [15, 16, 23, 30, 75, 78, 82, 86]
  },
  {
    "explanation": "AI robotics developments: tweets discussing humanoid robotics, robotic dogs, and new techniques for robotic control.",
    "tweetIndicies": [21, 24, 26, 28]
  },
  {
    "explanation": "New LLM or AI model announcements from various organizations, including Mistral, Salesforce, OpenAI Tasks, and Flex.1.",
    "tweetIndicies": [7, 19, 20, 22, 25, 27, 52, 73]
  },
  {
    "explanation": "Summaries or spotlights on recent AI papers: includes lists of new research on RAG, ChemAgent, Self-Adaptive LLMs, and more.",
    "tweetIndicies": [39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 57, 58, 60]
  },
  {
    "explanation": "RAG and LangChain usage tutorials: tweets sharing how to build chatbots, create hybrid search, or integrate RAG with production apps.",
    "tweetIndicies": [2, 3, 11, 32, 61, 101, 105]
  },
  {
    "explanation": "LangGraph or multi-agent frameworks: tweets covering specialized agent orchestration, GPT-based assistants, and multi-agent experiments.",
    "tweetIndicies": [8, 18, 29, 54, 104]
  },
  {
    "explanation": "LLM training, tokenization, and recommended resources: includes SFT/DPO, BPE/Tokenizer projects, pretraining on Apple Silicon, and educational material.",
    "tweetIndicies": [9, 37, 38, 50, 103]
  },
  {
    "explanation": "Experiences with Claude: remarks on Claude's scientific understanding and how it helps with writing and research analysis.",
    "tweetIndicies": [13, 14]
  },
  {
    "explanation": "Prompting and usage best practices: guidance on how to treat LLMs as report generators, focusing on what rather than how, the importance of context, or criticisms about coding with LLMs.",
    "tweetIndicies": [53, 67, 68, 102, 106]
  },
  {
    "explanation": "Podcasts on DeepSeek v3 and SGLang: references to discussions or interviews about these AI projects.",
    "tweetIndicies": [31, 69]
  },
  {
    "explanation": "LLM evaluation, correctness, and alignment: tweets about validation metrics, token accuracy vs loss, building evaluation systems, alignment concerns, and tests like Humanityâ€™s Last Exam.",
    "tweetIndicies": [10, 12, 34, 35, 36, 51, 55, 76, 79, 98, 99, 100]
  },
  {
    "explanation": "OpenAI FrontierMath drama: questions about whether OpenAI had access to special math benchmarks, and secrecy around the arrangement.",
    "tweetIndicies": [65, 66]
  },
  {
    "explanation": "R1 vs o1 drama: references to supposed model theft, pass@10 and performance comparisons, and confusion on how models truly work.",
    "tweetIndicies": [63, 74, 80, 81, 83]
  },
  {
    "explanation": "AI in education pilot: a study on using Copilot in a classroom, the substantial educational impact, and improvements on exams.",
    "tweetIndicies": [89, 90, 91, 92, 93, 94, 95]
  },
  {
    "explanation": "AI audio and music usage: tweets about combining AI with DAWs, TTS models like Kokoro, and creative audio workflows.",
    "tweetIndicies": [70, 71, 77, 85]
  },
  {
    "explanation": "Miscellaneous tweets that do not fit into larger topics but still mention AI or related commentary (ASCII art, enterprise security Q&A, personal viewpoints, knowledge gaps).",
    "tweetIndicies": [56, 59, 62, 64]
  }
]

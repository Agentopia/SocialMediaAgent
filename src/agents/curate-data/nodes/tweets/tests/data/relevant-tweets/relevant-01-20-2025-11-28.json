[
  {
    "id": "1881420706694709671",
    "edit_history_tweet_ids": ["1881420706694709671"],
    "in_reply_to_user_id": "1141052916570214400",
    "text": "Paper: https://t.co/z55eucxOMl\n\nModels: https://t.co/aJ5jMHDJLm\n\nRead the paper if you are interested in reasoning models. It is very well written and easy to understand. 🤗",
    "author_id": "1141052916570214400",
    "created_at": "2025-01-20T19:17:06.000Z",
    "referenced_tweets": [
      {
        "type": "replied_to",
        "id": "1881420703721009192"
      }
    ]
  },
  {
    "id": "1881420703721009192",
    "edit_history_tweet_ids": ["1881420703721009192"],
    "attachments": {
      "media_keys": ["3_1881420656350253056"]
    },
    "note_tweet": {
      "text": "Reinforcement Learning is all you need! @deepseek_ai R1 an open model that rivals @OpenAI o1 and other models on complex reasoning tasks just got released. But how is it trained? 👀  DeepSeek combines reinforcement learning with multi-stage training to achieve reasoning abilities that rival leading closed-source: Base → RL → SFT → RL → SFT → RL\n\n0/4 Base → RL: Uses GRPO on ended reasoning text-completions with rule-based Reward Models, e.g. Format, Math, Coding. Leading to coherent long reasoning/reflection CoT but with readability issues.\n\n1/4  RL → SFT: Collect up to 10k token long CoT using the previous model and few-shot prompting, then supervise fine-tune it. Leading readable thoughts and structured outputs (<think>, <summary>).\n\n2/4  SFT → RL: Same pipeline as on 0/4 with GRPO focusing on reasoning-intensive tasks (coding, mathematics, science, still rule-based) with additional “language consistency” reward. Leading coherent, readable performance on reasoning tasks\n\n3/4 RL → SFT: Collect 600k synthetic samples using Reject Sampling with the model (2/4) focusing on writing, role-playing, and other general-purpose tasks using DS v3 as LLM Judge. Adding additionally 200k samples for factual QA and translation from DS 3 training.\n\n4/4 SFT → RL: Use GRPO to improve helpfulness and harmlessness for reasoning tasks. Use rule-based RM for general tasks and use outcome RMs. Leading to DS R1 model\n\nInsights\n❌ No, MCTS for search and synthetic data or Process Reward Models (PRM) used\n🧠 RL on Base model lead to “aha moment”, where models learns to coherent “reasoning” longer outputs\n👀 Pure Reinforcement Learning can lead to strong reasoning abilities in LLMs.\n🚀 SFT before RL can accelerate and stabilize training.\n6️⃣ Trained 6 additional open models including Llama and Qwen on 800k synthetic samples from R1\n🏆 Distilled smaller models outperform previous versions.\n📈 DeepSeek-R1 achieves comparable performance to OpenAI-o1-1217 on reasoning benchmarks like AIME 2024 and MATH-500.\n🎯 Rule-based rewards for accuracy and format prove more effective than complex reward modeling\n🧮 Excels particularly in STEM tasks and long-context question answering",
      "entities": {
        "mentions": [
          {
            "start": 40,
            "end": 52,
            "username": "deepseek_ai",
            "id": "1714580962569588736"
          },
          {
            "start": 82,
            "end": 89,
            "username": "OpenAI",
            "id": "4398626122"
          }
        ]
      }
    },
    "text": "Reinforcement Learning is all you need! @deepseek_ai R1 an open model that rivals @OpenAI o1 and other models on complex reasoning tasks just got released. But how is it trained? 👀  DeepSeek combines reinforcement learning with multi-stage training to achieve reasoning abilities… https://t.co/1NlPD7DxSL https://t.co/qcmS2Jlzam",
    "author_id": "1141052916570214400",
    "created_at": "2025-01-20T19:17:05.000Z"
  },
  {
    "id": "1881416406673109399",
    "edit_history_tweet_ids": ["1881416406673109399"],
    "attachments": {
      "media_keys": ["3_1881416404370386944"]
    },
    "note_tweet": {
      "text": "AI is set to drive the creation of 11 million jobs by 2030, as highlighted in the @wef Future of Jobs Report 2025. \n\nRoles like AI specialists and big data experts are at the forefront of this shift, reflecting the growing demand for cutting-edge skills. \n\nAccess the report here: https://t.co/jAhanGz3XC",
      "entities": {
        "mentions": [
          {
            "start": 82,
            "end": 86,
            "username": "wef",
            "id": "5120691"
          }
        ],
        "urls": [
          {
            "start": 281,
            "end": 304,
            "url": "https://t.co/jAhanGz3XC",
            "expanded_url": "https://hubs.la/Q0338VJ_0",
            "display_url": "hubs.la/Q0338VJ_0"
          }
        ]
      }
    },
    "text": "AI is set to drive the creation of 11 million jobs by 2030, as highlighted in the @wef Future of Jobs Report 2025. \n\nRoles like AI specialists and big data experts are at the forefront of this shift, reflecting the growing demand for cutting-edge skills. \n\nAccess the report here:… https://t.co/vNU68HEj23 https://t.co/eK3nytejRY",
    "author_id": "992153930095251456",
    "created_at": "2025-01-20T19:00:01.000Z"
  },
  {
    "id": "1881415144825340391",
    "edit_history_tweet_ids": ["1881415144825340391"],
    "note_tweet": {
      "text": "I've heard enough sources say now that software engineering will be automated by the end of this year that it seems pretty clear to me that this will happen. \n\nWhen I tell people outside of labs this, they are still surprised or skeptical. This seems to be because they imagine that \"automated software engineering\" means large layoffs, and 0 people employed as software engineers in 2026. \n\nThat's not what it means. \n\nAutomated software engineering by the end of 2025 means no more interns, no more new grads. It means Cursor getting to $100M in revenue with ~ 30 people. It's the large workforce that didnt happen. Companies will still fight like crazy to hire great engineers. \n\nAutomated software engineering by the end of 2025 doesn't mean that Devin will build Cursor from scratch, with 0 people. It means that Devin will fix bugs and take on bigger and bigger issues while you make breakfast, making code happen in times where it wouldn't have otherwise. \n\nNew software eng grads have about 1-2 years to get as much real world experience as they can and otherwise demonstrate outlier ability. \n\nCompanies whose success depends on tech companies hiring lots of people got some stuff to figure out."
    },
    "text": "I've heard enough sources say now that software engineering will be automated by the end of this year that it seems pretty clear to me that this will happen. \n\nWhen I tell people outside of labs this, they are still surprised or skeptical. This seems to be because they imagine… https://t.co/Ec3qQeQHy7",
    "author_id": "64183939",
    "created_at": "2025-01-20T18:55:00.000Z"
  },
  {
    "id": "1881412831306002897",
    "edit_history_tweet_ids": ["1881412831306002897"],
    "attachments": {
      "media_keys": ["3_1881412685255905280"]
    },
    "note_tweet": {
      "text": "Summary of the DeepSeek models released today!\n\nDeepSeek-R1-Zero\n\n> Base Model: DeepSeek-V3-Base\n> Training Approach: Pure reinforcement learning (RL) without any supervised fine-tuning (SFT) as a preliminary step\n> RL Algorithm: Group Relative Policy Optimization (GRPO), which foregoes the critic model and estimates the baseline from group scores\n> Reward Modeling: Uses a rule-based reward system with accuracy rewards (for correct responses) and format rewards (for adhering to specified output formats)\n> Training Template: A simple template that requires the model to produce a reasoning process followed by the final answer, enclosed in specific tags\n> Performance: Achieves significant improvements in reasoning benchmarks, such as increasing the pass@1 score on AIME 2024 from 15.6% to 71.0%\n> Challenges: Faces issues like poor readability and language mixing\n\nDeepSeek-R1\n\n> Base Model: DeepSeek-V3-Base\n> Training Approach: Incorporates multi-stage training and cold-start data before RL\n> Cold Start: Collects thousands of long Chain-of-Thought (CoT) data to fine-tune the model as the initial RL actor\n> Reasoning-oriented RL: Applies large-scale RL training to enhance reasoning capabilities, focusing on tasks like coding, mathematics, and logic reasoning\n> Rejection Sampling and SFT: Uses rejection sampling to collect SFT data from the RL checkpoint, combined with supervised data from DeepSeek-V3 in various domains\n> Secondary RL Stage: Implements a secondary RL stage to align the model with human preferences, improving helpfulness and harmlessness while refining reasoning capabilities\n\n> Performance: Achieves performance comparable to OpenAI-o1-1217 on reasoning tasks and excels in various benchmarks, including MMLU, MATH-500, and Codeforces\n\nDistillation: Smaller Models\n\n> Base Models: Qwen2.5 and Llama series (1.5B, 7B, 8B, 14B, 32B, 70B)\n> Training Approach: Direct distillation from DeepSeek-R1 to smaller dense models using 800k curated samples\n> Performance: Distilled models, such as R1-Distill-Qwen-7B and R1-Distill-Qwen-32B, outperform non-reasoning models like GPT-4o-0513 and set new records on reasoning benchmark"
    },
    "text": "Summary of the DeepSeek models released today!\n\nDeepSeek-R1-Zero\n\n&gt; Base Model: DeepSeek-V3-Base\n&gt; Training Approach: Pure reinforcement learning (RL) without any supervised fine-tuning (SFT) as a preliminary step\n&gt; RL Algorithm: Group Relative Policy Optimization (GRPO), which… https://t.co/RK3kUHAXhf https://t.co/Iv1hduuVdM",
    "author_id": "874987512850128897",
    "created_at": "2025-01-20T18:45:48.000Z"
  },
  {
    "id": "1881412277053919426",
    "edit_history_tweet_ids": ["1881412277053919426"],
    "in_reply_to_user_id": "1641378826537295874",
    "text": "@deepseek_ai DeepSeek-R1 technical report\nhttps://t.co/VePF3wPZbb",
    "author_id": "1641378826537295874",
    "created_at": "2025-01-20T18:43:36.000Z",
    "referenced_tweets": [
      {
        "type": "replied_to",
        "id": "1881412232204206186"
      }
    ]
  },
  {
    "id": "1881412271236346233",
    "edit_history_tweet_ids": ["1881412119217987680", "1881412271236346233"],
    "attachments": {
      "media_keys": ["7_1881411953492692993"]
    },
    "text": "DeepSeek R1 671B running on 2 M2 Ultras faster than reading speed.\n\nGetting close to open-source O1, at home, on consumer hardware.\n\nWith mlx.distributed and mlx-lm, 3-bit quantization (~4 bpw) https://t.co/RnkYxwZG3c",
    "author_id": "245262377",
    "created_at": "2025-01-20T18:43:35.000Z"
  },
  {
    "id": "1881412232204206186",
    "edit_history_tweet_ids": ["1881412232204206186"],
    "in_reply_to_user_id": "1641378826537295874",
    "text": "@deepseek_ai DeepSeek-R1 release\nhttps://t.co/wHrPgRrSSm",
    "author_id": "1641378826537295874",
    "created_at": "2025-01-20T18:43:25.000Z",
    "referenced_tweets": [
      {
        "type": "quoted",
        "id": "1881318130334814301"
      },
      {
        "type": "replied_to",
        "id": "1881411458678014215"
      }
    ]
  },
  {
    "id": "1881412032559554648",
    "edit_history_tweet_ids": ["1881412032559554648"],
    "attachments": {
      "media_keys": ["3_1881411317736554496"]
    },
    "text": "“perhaps the guinea pig can be used as a rope or something”\nR1-llama-8B has all the characteristic fretful silliness of R1-preview. it's not much use but you can see the diversity that only free exploration could unearth. It remembers your silly riddles. It's not a Riddler model. https://t.co/86eX8yUbDt",
    "author_id": "192201556",
    "created_at": "2025-01-20T18:42:38.000Z"
  },
  {
    "id": "1881411595974369357",
    "edit_history_tweet_ids": ["1881411595974369357"],
    "note_tweet": {
      "text": "most people don't know that lex fridman's bizarre character arc actually started in research.  here's the real story:\n\n> around 2018 tesla deploys Autopilot, software that lets the car drive for you on highways. but only as your hands are on the steering wheel\n> all research finds that people stop paying attention  when they are just holding their hands on a wheel without doing anything\n> one person wrote a paper saying that actually, contrary to every other finding –  humans do pay attention while sitting there and doing nothing! \n> that person is lex fridman\n> turns out the paper only used a sample of n=21, and was generally criticized for lack of rigor\n> lex somehow still leverages this into a friendship with elon (still unsure how)\n> lex gets elon to come on his \"AI Podcast\" in like the third episode\n> lex uses his part-time research position at MIT to cosplay as a professor online\n> lex gets a lot more well-known guests to come on the pod\n> now podcast is everywhere and lex wants to befriend putin\n\nps. the driver monitoring paper is now scrubbed from the internet"
    },
    "text": "most people don't know that lex fridman's bizarre character arc actually started in research.  here's the real story:\n\n&gt; around 2018 tesla deploys Autopilot, software that lets the car drive for you on highways. but only as your hands are on the steering wheel\n&gt; all research… https://t.co/mQ9GqUGCdw https://t.co/C7hb7U29IS",
    "author_id": "783098774130401280",
    "created_at": "2025-01-20T18:40:54.000Z",
    "referenced_tweets": [
      {
        "type": "quoted",
        "id": "1881245844122570903"
      }
    ]
  },
  {
    "id": "1881411500151386566",
    "edit_history_tweet_ids": ["1881411500151386566"],
    "attachments": {
      "media_keys": ["3_1881410801099055104"]
    },
    "text": "Maybe the most interesting section of the @deepseek_ai paper!!\n\n&gt; No MCTS for search and data generation\n&gt; No Process Reward Models (PRM) for RL https://t.co/u0CHijM1xY",
    "author_id": "1141052916570214400",
    "created_at": "2025-01-20T18:40:31.000Z"
  },
  {
    "id": "1881411458678014215",
    "edit_history_tweet_ids": ["1881411458678014215"],
    "attachments": {
      "media_keys": ["3_1881407238197407745"]
    },
    "note_tweet": {
      "text": "DeepSeek-R1 is now in the Arena🔥\n\nCongrats @deepseek_ai on R1 release! An open reasoning model matching OpenAI o1 in hard benchmarks like GPQA/SWE-Bench/AIME!\n\nNow for the real-world challenge—R1 is in https://t.co/azF9dwf43Y for human evaluation. Bring your toughest prompts and challenge it!",
      "entities": {
        "mentions": [
          {
            "start": 43,
            "end": 55,
            "username": "deepseek_ai",
            "id": "1714580962569588736"
          }
        ],
        "urls": [
          {
            "start": 202,
            "end": 225,
            "url": "https://t.co/azF9dwf43Y",
            "expanded_url": "http://lmarena.ai",
            "display_url": "lmarena.ai"
          }
        ]
      }
    },
    "text": "DeepSeek-R1 is now in the Arena🔥\n\nCongrats @deepseek_ai on R1 release! An open reasoning model matching OpenAI o1 in hard benchmarks like GPQA/SWE-Bench/AIME!\n\nNow for the real-world challenge—R1 is in https://t.co/gxIFU9kIc2 for human evaluation. Bring your toughest prompts and… https://t.co/qfUw9QFUa7 https://t.co/UnJHdwcDsP",
    "author_id": "1641378826537295874",
    "created_at": "2025-01-20T18:40:21.000Z"
  },
  {
    "id": "1881410470713954788",
    "edit_history_tweet_ids": ["1881410278568730971", "1881410470713954788"],
    "text": "Excited to share we are hiring for a new type of scholar role. \n\nAlgorithm interface scholars will re-imagine how humans interact with algorithms. \n\nThese scholars will lead the frontier in UI-algorithm co-design. 🔥\n\nJoin us at @CohereForAI @cohere ✨\n\nhttps://t.co/w9wYXGi8iG",
    "author_id": "731538535795163136",
    "created_at": "2025-01-20T18:36:25.000Z"
  },
  {
    "id": "1881406988833415529",
    "edit_history_tweet_ids": ["1881406988833415529"],
    "text": "incredible work, amazing results, and yet another reminder of how disappointingly boring and narrow the scope of typical evals is\n\n(multiple choice tests, narrow code gen, and math word problems *yawn*) https://t.co/bkrWAOMDfg",
    "author_id": "1605274291569799168",
    "created_at": "2025-01-20T18:22:35.000Z",
    "referenced_tweets": [
      {
        "type": "quoted",
        "id": "1881316758218535039"
      }
    ]
  },
  {
    "id": "1881405358931059004",
    "edit_history_tweet_ids": ["1881405358931059004"],
    "attachments": {
      "media_keys": ["3_1881405013378949120"]
    },
    "text": "this is insane 🤯\n\nDeepSeek-R1 Coder is now available in anychat\n\nits like cursor or v0 in the browser\n\nmade a tic tac toe game in seconds\n\nwill try to keep it working as long as I have credits 😀 https://t.co/YFtqP4D9Xw",
    "author_id": "2465283662",
    "created_at": "2025-01-20T18:16:07.000Z"
  },
  {
    "id": "1881404791181750338",
    "edit_history_tweet_ids": ["1881404791181750338"],
    "text": "RT @weaviate_io: Looking for the best embedding model for your documents mixed with images and text?\n\nWeaviate now supports @VoyageAI’s mul…",
    "author_id": "95016894",
    "created_at": "2025-01-20T18:13:51.000Z",
    "referenced_tweets": [
      {
        "type": "retweeted",
        "id": "1881386264697397632"
      }
    ]
  },
  {
    "id": "1881401836483252705",
    "edit_history_tweet_ids": ["1881401836483252705"],
    "note_tweet": {
      "text": "The past few years of LLM research show that if you can (i) define a meaningful objective and (ii) leave enough \"room\" for the system to fit it, like CoT, tool use, or multi-stage modules, we have plenty of ways of fitting your downstream signal.\n\nThe hard part is defining the objective & the scaffolding of that \"room\". That's why DSPy exists: you handle the program and the metric; you delegate the fitting of prompts or weights."
    },
    "text": "The past few years of LLM research show that if you can (i) define a meaningful objective and (ii) leave enough \"room\" for the system to fit it, like CoT, tool use, or multi-stage modules, we have plenty of ways of fitting your downstream signal.\n\nThe hard part is defining the… https://t.co/hE8vNJFRd9 https://t.co/XCul7uwtJ2",
    "author_id": "1605274291569799168",
    "created_at": "2025-01-20T18:02:07.000Z",
    "referenced_tweets": [
      {
        "type": "quoted",
        "id": "1881398258318369136"
      }
    ]
  },
  {
    "id": "1881399873775489076",
    "edit_history_tweet_ids": ["1881399873775489076"],
    "note_tweet": {
      "text": "Benchmarks were really key in the evolution of systems research and in ML when key research was about core methods (think optimizers or architectures).\n\nAt this point now, benchmarking is starting to be counterproductive. Yes, o1 impressed people by being good at some math benchmarks, but replicating that win doesn't actually replicate o1."
    },
    "text": "Benchmarks were really key in the evolution of systems research and in ML when key research was about core methods (think optimizers or architectures).\n\nAt this point now, benchmarking is starting to be counterproductive. Yes, o1 impressed people by being good at some math… https://t.co/VCCrebHapK https://t.co/XCul7uwtJ2",
    "author_id": "1605274291569799168",
    "created_at": "2025-01-20T17:54:19.000Z",
    "referenced_tweets": [
      {
        "type": "quoted",
        "id": "1881398258318369136"
      }
    ]
  },
  {
    "id": "1881399579079512502",
    "edit_history_tweet_ids": ["1881399579079512502"],
    "text": "RT @rtk254: Video models != world models \n\n\"We find that across a range of current models (Sora, Runway, Pika, Lumiere, Stable Video Diffus…",
    "author_id": "10834752",
    "created_at": "2025-01-20T17:53:09.000Z",
    "referenced_tweets": [
      {
        "type": "retweeted",
        "id": "1881096518234726555"
      }
    ]
  },
  {
    "author_id": "800854096219471872",
    "note_tweet": {
      "text": "This \"Aha moment\" in the DeepSeek-R1 paper is huge:\n\nPure reinforcement learning (RL) enables an LLM to automatically learn to think and reflect.\n\nThis challenges the prior belief that replicating OpenAI's o1 reasoning models requires extensive CoT data. It turns out you just need to give it the right incentives.\n\nWe are so back in the AlphaGo excitement era: by playing countless Go games and maximizing the reward function (winning the game) using pure RL, AlphaGo beat the best human players.\n\nNow we are entering the LLM RL era.\n2025 could be the year of RL."
    },
    "text": "This \"Aha moment\" in the DeepSeek-R1 paper is huge:\n\nPure reinforcement learning (RL) enables an LLM to automatically learn to think and reflect.\n\nThis challenges the prior belief that replicating OpenAI's o1 reasoning models requires extensive CoT data. It turns out you just… https://t.co/dIJ3l1XCGk https://t.co/5qBR75LD0n",
    "id": "1881398653845410286",
    "created_at": "2025-01-20T17:49:28.000Z",
    "edit_history_tweet_ids": ["1881398653845410286"],
    "attachments": {
      "media_keys": ["3_1881392953555615744"]
    }
  },
  {
    "author_id": "1605274291569799168",
    "text": "Excitement is justified, but the most reinforced takeaways from R1 seem to be that:\n\n- If you know how to measure a downstream outcome, you should RL your way out of it. (It's basically a many-task STaR?)\n\n- The math and coding tasks o1 benchmarked on aren't all that difficult.",
    "id": "1881398258318369136",
    "created_at": "2025-01-20T17:47:54.000Z",
    "edit_history_tweet_ids": ["1881398258318369136"]
  },
  {
    "author_id": "2465283662",
    "note_tweet": {
      "text": "its now available in ai-gradio      \n\npip install --upgrade \"ai-gradio[deepseek]\"    \n\nimport gradio as gr   \nimport ai_gradio      \n\ngr.load(name='deepseek:deepseek-reasoner', src=ai_gradio.registry).launch()    \n\ngithub: https://t.co/f2s3ck8OL9 \n\nquick start colab: https://t.co/j6kyZOfoAb",
      "entities": {
        "urls": [
          {
            "start": 223,
            "end": 246,
            "url": "https://t.co/f2s3ck8OL9",
            "expanded_url": "https://github.com/AK391/ai-gradio",
            "display_url": "github.com/AK391/ai-gradio"
          },
          {
            "start": 268,
            "end": 291,
            "url": "https://t.co/j6kyZOfoAb",
            "expanded_url": "https://colab.research.google.com/drive/1Ilz8LRQp0lQHxXu26PEJkXtXmvj1MrFe?usp=sharing",
            "display_url": "colab.research.google.com/drive/1Ilz8LRQ…"
          }
        ]
      }
    },
    "text": "@skirano its now available in ai-gradio      \n\npip install --upgrade \"ai-gradio[deepseek]\"    \n\nimport gradio as gr   \nimport ai_gradio      \n\ngr.load(name='deepseek:deepseek-reasoner', src=ai_gradio.registry).launch()    \n\ngithub: https://t.co/vy68ARTizv \n\nquick start colab:… https://t.co/EYjPRSVjb1 https://t.co/2q5rXWBKx3",
    "id": "1881398089740947629",
    "referenced_tweets": [
      {
        "type": "replied_to",
        "id": "1881351047312191960"
      }
    ],
    "in_reply_to_user_id": "73105934",
    "created_at": "2025-01-20T17:47:13.000Z",
    "edit_history_tweet_ids": ["1881398089740947629"],
    "attachments": {
      "media_keys": ["3_1881398023667716097"]
    }
  },
  {
    "author_id": "2465283662",
    "note_tweet": {
      "text": "its now available in ai-gradio      \n\npip install --upgrade \"ai-gradio[deepseek]\"    \n\nimport gradio as gr   \nimport ai_gradio      \n\ngr.load(name='deepseek:deepseek-reasoner', src=ai_gradio.registry, ).launch()    \n\ngithub: https://t.co/f2s3ck8gVB \n\nquick start colab: https://t.co/j6kyZOeQKD",
      "entities": {
        "urls": [
          {
            "start": 225,
            "end": 248,
            "url": "https://t.co/f2s3ck8gVB",
            "expanded_url": "https://github.com/AK391/ai-gradio",
            "display_url": "github.com/AK391/ai-gradio"
          },
          {
            "start": 270,
            "end": 293,
            "url": "https://t.co/j6kyZOeQKD",
            "expanded_url": "https://colab.research.google.com/drive/1Ilz8LRQp0lQHxXu26PEJkXtXmvj1MrFe?usp=sharing",
            "display_url": "colab.research.google.com/drive/1Ilz8LRQ…"
          }
        ]
      }
    },
    "text": "@Teknium1 its now available in ai-gradio      \n\npip install --upgrade \"ai-gradio[deepseek]\"    \n\nimport gradio as gr   \nimport ai_gradio      \n\ngr.load(name='deepseek:deepseek-reasoner', src=ai_gradio.registry, ).launch()    \n\ngithub: https://t.co/vy68ARSKJX \n\nquick start colab:… https://t.co/BbMg5HziVw https://t.co/iFXVZByWmC",
    "id": "1881397550072152358",
    "referenced_tweets": [
      {
        "type": "replied_to",
        "id": "1881316758218535039"
      }
    ],
    "in_reply_to_user_id": "1365020011123773442",
    "created_at": "2025-01-20T17:45:05.000Z",
    "edit_history_tweet_ids": ["1881397550072152358"],
    "attachments": {
      "media_keys": ["3_1881397520946855940"]
    }
  },
  {
    "author_id": "2465283662",
    "note_tweet": {
      "text": "its now available in ai-gradio    \n\npip install --upgrade \"ai-gradio[deepseek]\"  \n\nimport gradio as gr  \nimport ai_gradio    \n\ngr.load(name='deepseek:deepseek-reasoner', src=ai_gradio.registry, ).launch()  \n\ngithub: https://t.co/f2s3ck8OL9\n\nquick start colab: https://t.co/j6kyZOfoAb",
      "entities": {
        "urls": [
          {
            "start": 216,
            "end": 239,
            "url": "https://t.co/f2s3ck8OL9",
            "expanded_url": "https://github.com/AK391/ai-gradio",
            "display_url": "github.com/AK391/ai-gradio"
          },
          {
            "start": 260,
            "end": 283,
            "url": "https://t.co/j6kyZOfoAb",
            "expanded_url": "https://colab.research.google.com/drive/1Ilz8LRQp0lQHxXu26PEJkXtXmvj1MrFe?usp=sharing",
            "display_url": "colab.research.google.com/drive/1Ilz8LRQ…"
          }
        ]
      }
    },
    "text": "@AravSrinivas its now available in ai-gradio    \n\npip install --upgrade \"ai-gradio[deepseek]\"  \n\nimport gradio as gr  \nimport ai_gradio    \n\ngr.load(name='deepseek:deepseek-reasoner', src=ai_gradio.registry, ).launch()  \n\ngithub: https://t.co/vy68ARTizv\n\nquick start colab:… https://t.co/xII2r8xiok https://t.co/vcZhxtMy6h",
    "id": "1881396944163188840",
    "referenced_tweets": [
      {
        "type": "replied_to",
        "id": "1881372861405036773"
      }
    ],
    "in_reply_to_user_id": "759894532649545732",
    "created_at": "2025-01-20T17:42:40.000Z",
    "edit_history_tweet_ids": ["1881396944163188840"],
    "attachments": {
      "media_keys": ["3_1881396832208613376"]
    }
  },
  {
    "author_id": "731538535795163136",
    "text": "I also think these human evals highlight how it is very possible for a 32B open-weights model to outperform much larger models like Claude, Mistral Large 2, Llama 405B. \n\nGetting away with scale alone is no longer enough.",
    "id": "1881396759295045680",
    "referenced_tweets": [
      {
        "type": "replied_to",
        "id": "1881391184377160161"
      }
    ],
    "in_reply_to_user_id": "731538535795163136",
    "created_at": "2025-01-20T17:41:56.000Z",
    "edit_history_tweet_ids": ["1881396759295045680"]
  },
  {
    "author_id": "731538535795163136",
    "text": "This has further convinced me of the need for private third-party evals. \n\nOn many academic benchmarks, this spread in perf is not visible because models saturate perf due to test set contagion.\n\nIn contrast, these private human evals are way closer to our internal estimates. https://t.co/2rBkw63bRV",
    "id": "1881391184377160161",
    "referenced_tweets": [
      {
        "type": "quoted",
        "id": "1881386848397975688"
      }
    ],
    "created_at": "2025-01-20T17:19:47.000Z",
    "edit_history_tweet_ids": ["1881391184377160161"]
  },
  {
    "author_id": "610552974",
    "text": "RT @CohereForAI: On @scale_AI's private multilingual protocol, Aya Expanse is indexed as the best open-weights model\n\nIn some languages we'…",
    "id": "1881390269020721505",
    "referenced_tweets": [
      {
        "type": "retweeted",
        "id": "1881386848397975688"
      }
    ],
    "created_at": "2025-01-20T17:16:09.000Z",
    "edit_history_tweet_ids": ["1881390269020721505"]
  },
  {
    "author_id": "13614262",
    "text": "Quick colab on using Deepseek-R1 to generate data: https://t.co/9tNZEBaMmx\n\nLet me know if you guys want a full-fledged example of deepseek-distillation: generate data and fine-tune a model. https://t.co/zV1HE7sufa",
    "id": "1881388860602745116",
    "referenced_tweets": [
      {
        "type": "replied_to",
        "id": "1881379171299123661"
      }
    ],
    "in_reply_to_user_id": "13614262",
    "created_at": "2025-01-20T17:10:33.000Z",
    "edit_history_tweet_ids": ["1881388860602745116"],
    "attachments": {
      "media_keys": ["3_1881388646496161792"]
    }
  },
  {
    "author_id": "3378986176",
    "text": "RT @natolambert: For those trying to understand DeepSeeks Group Relative Policy Optimization (GRPO): GRPO is just PPO without a value funct…",
    "id": "1881388823999070456",
    "referenced_tweets": [
      {
        "type": "retweeted",
        "id": "1881380809153847711"
      }
    ],
    "created_at": "2025-01-20T17:10:24.000Z",
    "edit_history_tweet_ids": ["1881388823999070456"]
  },
  {
    "author_id": "192201556",
    "text": "I'm inclined to say that the next Big Thing is, indeed, multi-agent training. You can't do \"honest\" RL for agentic and multi-turn performance without it. You need a DeepSeek-Prompter pestering DeepSeek-Solver, in a tight loop, and with async tools.\nRLHF dies in 2025.",
    "id": "1881387194004467964",
    "created_at": "2025-01-20T17:03:56.000Z",
    "edit_history_tweet_ids": ["1881387194004467964"]
  },
  {
    "author_id": "245262377",
    "text": "Wow, DeepSeek R1 Distill Qwen 7B (in 4-bit) nailed the first hard math question I asked it.\n\nThought for ~3200 tokens in about 35 seconds on M4 Max with mlx-lm. https://t.co/ZQ3lLnKn5x",
    "id": "1881386796266946743",
    "created_at": "2025-01-20T17:02:21.000Z",
    "edit_history_tweet_ids": ["1881386796266946743"],
    "attachments": {
      "media_keys": ["7_1881386527739310080"]
    }
  },
  {
    "author_id": "1007413134",
    "note_tweet": {
      "text": "That a *second* paper dropped with tons of RL flywheel secrets and *multimodal* o1-style reasoning is not on my bingo card today. Kimi's (another startup) and DeepSeek's papers remarkably converged on similar findings:\n\n> No need for complex tree search like MCTS. Just linearize the thought trace and do good old autoregressive prediction;\n> No need for value functions that require another expensive copy of the model;\n> No need for dense reward modeling. Rely as much as possible on groundtruth, end result. \n\nDifferences:\n\n> DeepSeek does AlphaZero approach - purely bootstrap through RL w/o human input, i.e. \"cold start\". Kimi does AlphaGo-Master approach: light SFT to warm up through prompt-engineered CoT traces.\n> DeepSeek weights are MIT license (thought leadership!); Kimi does not have a model release yet.\n> Kimi shows strong multimodal performance (!) on benchmarks like MathVista, which requires visual understanding of geometry, IQ tests, etc.\n> Kimi paper has a LOT more details on the system design: RL infrastructure, hybrid cluster, code sandbox, parallelism strategies; and learning details: long context, CoT compression, curriculum, sampling strategy, test case generation, etc.\n\nUpbeat reads on a holiday!"
    },
    "text": "That a *second* paper dropped with tons of RL flywheel secrets and *multimodal* o1-style reasoning is not on my bingo card today. Kimi's (another startup) and DeepSeek's papers remarkably converged on similar findings:\n\n&gt; No need for complex tree search like MCTS. Just linearize… https://t.co/GgZ18jb3Vv https://t.co/NrV2WyunC9",
    "id": "1881382618627019050",
    "created_at": "2025-01-20T16:45:45.000Z",
    "edit_history_tweet_ids": ["1881382618627019050"],
    "attachments": {
      "media_keys": ["3_1881378108139601920"]
    }
  },
  {
    "author_id": "992153930095251456",
    "text": "Nvidia presented Project Digits, a personal supercomputer designed for students, developers, and researchers to run, train, and fine-tune large AI models locally. \n\nLearn more in The Batch: https://t.co/PAlBvRZXOR https://t.co/2SgQrsNHui",
    "id": "1881380064731832466",
    "created_at": "2025-01-20T16:35:36.000Z",
    "edit_history_tweet_ids": ["1881380064731832466"],
    "attachments": {
      "media_keys": ["3_1881380062978576385"]
    }
  },
  {
    "author_id": "13614262",
    "note_tweet": {
      "text": "Deepseek has done it again! This time, lots of action packed insights, stuff that the top labs are not willing to share.\n\nSome insights:\n\n1. \"We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step.\" \n\nThis will take some time to sink in, given how clearly it's been laid out. This has been known in a few circles (@natolambert has also talked about this many times), but lots of people don't seem to get the magnitude of this. \n\nBut of course, note that RL is great for reasoning, and there are many problems where reasoning is not needed or not useful and SFT is great.",
      "entities": {
        "mentions": [
          {
            "start": 393,
            "end": 405,
            "username": "natolambert",
            "id": "2939913921"
          }
        ]
      }
    },
    "text": "Deepseek has done it again! This time, lots of action packed insights, stuff that the top labs are not willing to share.\n\nSome insights:\n\n1. \"We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step.\"… https://t.co/1ia1YhF0DB https://t.co/cgRGtNkJuM https://t.co/7Nv2HtJY4L",
    "id": "1881378064401334399",
    "referenced_tweets": [
      {
        "type": "quoted",
        "id": "1881318130334814301"
      }
    ],
    "created_at": "2025-01-20T16:27:39.000Z",
    "edit_history_tweet_ids": ["1881378064401334399"],
    "attachments": {
      "media_keys": ["3_1881376451066568704"]
    }
  },
  {
    "author_id": "874987512850128897",
    "text": "Try out R1 Distill Qwen 1.5B in a FREE Google Colab! 🔥\n\nThe vibes are looking gooood! https://t.co/dXvYLoOFuk https://t.co/IQApPfEjzG",
    "id": "1881377609445167380",
    "referenced_tweets": [
      {
        "type": "quoted",
        "id": "1881319500089634954"
      }
    ],
    "created_at": "2025-01-20T16:25:51.000Z",
    "edit_history_tweet_ids": ["1881377609445167380"],
    "attachments": {
      "media_keys": ["7_1881377422135676928"]
    }
  },
  {
    "author_id": "1718879852827484160",
    "text": "Likely that Anthropic has a reasoner but they simply dont have the compute to serve it if they are already facing limits now https://t.co/HTFTfsIIPK",
    "id": "1881375436644741410",
    "referenced_tweets": [
      {
        "type": "quoted",
        "id": "1881374280837165191"
      }
    ],
    "created_at": "2025-01-20T16:17:13.000Z",
    "edit_history_tweet_ids": ["1881375436644741410"]
  },
  {
    "author_id": "759894532649545732",
    "text": "It's kinda wild to see reasoning get commoditized this fast.  We should fully expect an o3 level model that's open-sourced by the end of the year, probably even mid-year. https://t.co/oyIXkS4uDM",
    "id": "1881375063246835910",
    "created_at": "2025-01-20T16:15:43.000Z",
    "edit_history_tweet_ids": ["1881375063246835910"],
    "attachments": {
      "media_keys": ["3_1881374834615009280"]
    }
  },
  {
    "id": "1881373192989602049",
    "attachments": {
      "media_keys": ["3_1881373043060002816"]
    },
    "text": "Introducing Kimi k1.5 \n\nan o1-level multi-modal model  \n\n-Sota short-CoT performance, outperforming GPT-4o and Claude Sonnet 3.5 on 📷AIME, 📷 LiveCodeBench by a large margin (up to +550%) \n\n-Long-CoT performance matches o1 across multiple modalities (📷MathVista, 📷Codeforces,… https://t.co/dgbCimJxxS https://t.co/9OnDIUEeBj",
    "edit_history_tweet_ids": ["1881373192989602049"],
    "created_at": "2025-01-20T16:08:18.000Z",
    "author_id": "2465283662",
    "note_tweet": {
      "text": "Introducing Kimi k1.5 \n\nan o1-level multi-modal model  \n\n-Sota short-CoT performance, outperforming GPT-4o and Claude Sonnet 3.5 on 📷AIME, 📷 LiveCodeBench by a large margin (up to +550%) \n\n-Long-CoT performance matches o1 across multiple modalities (📷MathVista, 📷Codeforces, etc)  Tech report: i-k1.5… \n\nKey ingredients of k1.5 -Long context scaling. Up to 128k tokens for RL generation. Efficient training with partial rollouts. \n\n-Improved policy optimization: online mirror descent, sampling strategies, length penalty, and others.\n\n-Multi modalities. Joint reasoning over text and vision."
    }
  },
  {
    "id": "1881373186744590406",
    "attachments": {
      "media_keys": ["3_1881373183162396672"]
    },
    "text": "Okay so this is so far the most important paper in AI of the year https://t.co/xzMoqYM9hj",
    "edit_history_tweet_ids": ["1881373186744590406"],
    "created_at": "2025-01-20T16:08:16.000Z",
    "author_id": "441465751"
  },
  {
    "id": "1881372868706988453",
    "text": "🤖 From this week's issue: Inspired by the human vocal tract, a new AI model can produce and understand vocal imitations of everyday sounds. The method could help build new sonic interfaces for entertainment and education. https://t.co/PTpNw0V165",
    "edit_history_tweet_ids": ["1881372868706988453"],
    "created_at": "2025-01-20T16:07:00.000Z",
    "author_id": "763368160527544320"
  },
  {
    "id": "1881372861405036773",
    "attachments": {
      "media_keys": ["3_1881372859013984256"]
    },
    "text": "DeepSeek has largely replicated o1-mini and has open sourced it. https://t.co/2TbQ5p5l2c",
    "edit_history_tweet_ids": ["1881372861405036773"],
    "created_at": "2025-01-20T16:06:59.000Z",
    "author_id": "759894532649545732"
  },
  {
    "id": "1881371108584161345",
    "attachments": {
      "media_keys": ["3_1881371105392304128"]
    },
    "text": "🤖 🔄 Build Smarter AI Agents\n\nLangGraph introduces state-aware AI workflows for sophisticated reasoning and decision-making. This tutorial shows you how to create advanced AI applications with powerful cyclical processing capabilities.\n\nLearn more: https://t.co/gisX3eURU5 https://t.co/a3Sw6LMulZ",
    "edit_history_tweet_ids": ["1881371108584161345"],
    "created_at": "2025-01-20T16:00:01.000Z",
    "author_id": "1589007443853340672"
  },
  {
    "id": "1881368754514231540",
    "attachments": {
      "media_keys": ["3_1881368618161635328"]
    },
    "note_tweet": {
      "entities": {
        "urls": [
          {
            "start": 351,
            "end": 374,
            "url": "https://t.co/TxFc1kst7B",
            "expanded_url": "https://msty.app/",
            "display_url": "msty.app"
          },
          {
            "start": 492,
            "end": 515,
            "url": "https://t.co/OLIthsQXps",
            "expanded_url": "https://newsletter.danielmiessler.com/subscribe",
            "display_url": "newsletter.danielmiessler.com/subscribe"
          },
          {
            "start": 632,
            "end": 655,
            "url": "https://t.co/hPHM6sAK78",
            "expanded_url": "https://discord.com/channels/1110206757227216916/shop?redirect_from=rez0",
            "display_url": "discord.com/channels/11102…"
          },
          {
            "start": 709,
            "end": 732,
            "url": "https://t.co/PBrZMF0vWd",
            "expanded_url": "https://cursor.sh/",
            "display_url": "cursor.sh"
          },
          {
            "start": 818,
            "end": 841,
            "url": "https://t.co/WClyiJCtGi",
            "expanded_url": "https://claude.ai",
            "display_url": "claude.ai"
          },
          {
            "start": 981,
            "end": 1004,
            "url": "https://t.co/R6YQbk6DL1",
            "expanded_url": "https://haizelabs.com/",
            "display_url": "haizelabs.com"
          },
          {
            "start": 1276,
            "end": 1299,
            "url": "https://t.co/RdqNa9dElm",
            "expanded_url": "https://www.dwarkeshpatel.com/p/leopold-aschenbrenner",
            "display_url": "dwarkeshpatel.com/p/leopold-asch…"
          },
          {
            "start": 1439,
            "end": 1462,
            "url": "https://t.co/DOjiKVT6On",
            "expanded_url": "https://samcurry.net/hacking-kia",
            "display_url": "samcurry.net/hacking-kia"
          },
          {
            "start": 1564,
            "end": 1587,
            "url": "https://t.co/mrci2Barc2",
            "expanded_url": "https://github.com/danielmiessler/fabric",
            "display_url": "github.com/danielmiessler…"
          },
          {
            "start": 1724,
            "end": 1747,
            "url": "https://t.co/HWhgHW0tsr",
            "expanded_url": "https://bughunters.google.com/",
            "display_url": "bughunters.google.com"
          },
          {
            "start": 1867,
            "end": 1890,
            "url": "https://t.co/OzUqLIdlcS",
            "expanded_url": "https://github.com/elder-plinius/L1B3RT4S/",
            "display_url": "github.com/elder-plinius/…"
          },
          {
            "start": 2097,
            "end": 2120,
            "url": "https://t.co/htk7WQ4uvl",
            "expanded_url": "https://www.whiterabbitneo.com/",
            "display_url": "whiterabbitneo.com"
          },
          {
            "start": 2252,
            "end": 2275,
            "url": "https://t.co/9t3ECVhLtP",
            "expanded_url": "https://www.oreilly.com/library/view/ai-engineering/9781098166298/",
            "display_url": "oreilly.com/library/view/a…"
          },
          {
            "start": 2439,
            "end": 2462,
            "url": "https://t.co/nhdjBUEOUQ",
            "expanded_url": "https://www.youtube.com/@matthew_berman",
            "display_url": "youtube.com/@matthew_berman"
          },
          {
            "start": 2575,
            "end": 2598,
            "url": "https://t.co/kFDNGLGdkr",
            "expanded_url": "https://olickel.com/everything-i-know-about-prompting-llms",
            "display_url": "olickel.com/everything-i-k…"
          },
          {
            "start": 2725,
            "end": 2748,
            "url": "https://t.co/8ViWEaEhPz",
            "expanded_url": "https://github.com/SouthBridgeAI/offmute",
            "display_url": "github.com/SouthBridgeAI/…"
          },
          {
            "start": 2802,
            "end": 2825,
            "url": "https://t.co/GSqFOAvtii",
            "expanded_url": "https://thacker.beehiiv.com/subscribe",
            "display_url": "thacker.beehiiv.com/subscribe"
          }
        ],
        "mentions": [
          {
            "start": 395,
            "end": 410,
            "username": "DanielMiessler",
            "id": "1543121"
          },
          {
            "start": 618,
            "end": 630,
            "username": "ctbbpodcast",
            "id": "1600519214519013377"
          },
          {
            "start": 871,
            "end": 881,
            "username": "haizelabs",
            "id": "1744848449403748352"
          },
          {
            "start": 1057,
            "end": 1069,
            "username": "dwarkesh_sp",
            "id": "1209960539390201864"
          },
          {
            "start": 1352,
            "end": 1360,
            "username": "samwcyo",
            "id": "825606932887134212"
          },
          {
            "start": 1616,
            "end": 1626,
            "username": "GoogleVRP",
            "id": "972044366205411330"
          },
          {
            "start": 1825,
            "end": 1839,
            "username": "elder_plinius",
            "id": "1656536425087500288"
          },
          {
            "start": 2304,
            "end": 2318,
            "username": "MatthewBerman",
            "id": "6681172"
          },
          {
            "start": 2530,
            "end": 2539,
            "username": "hrishioa",
            "id": "1548645654"
          },
          {
            "start": 2687,
            "end": 2696,
            "username": "hrishioa",
            "id": "1548645654"
          }
        ]
      },
      "text": "I sent this to my email list and people loved it, so I wanted to post it here. It's a completely subjective list of stuff I loved in 2024. I hope you love it too😊 \n\nBest MacOS Desktop App\nmsty - fantastic AI client that supports local and API usage of models. You can use multiple models at the same time. You can fork the conversation. It's awesome.\nhttps://t.co/TxFc1kst7B\n---\nBest Newsletter\n@DanielMiessler's Newsletter - He covers the best in cybersecurity and AI (kinda like me, haha).\nhttps://t.co/OLIthsQXps\n---\nBest Discord Channel\nCTBB \"Critical Thinkers\" Chat - direct access to the worlds top 300 hackers. @ctbbpodcast \nhttps://t.co/hPHM6sAK78\n---\nBest AI App\ncursor - the AI-powered code editor.\nhttps://t.co/PBrZMF0vWd\n---\nBest Model\nClaude Sonnet - simply because of how fast, good, and flexible it is.\nhttps://t.co/WClyiJCtGi\n---\nBest Red Teaming Company\n@haizelabs  - for their automated red teaming. (More detailed description written earlier in the newsletter.)\nhttps://t.co/R6YQbk6DL1\n---\nBest Podcast Episode\nThe Leopold episode on the @dwarkesh_sp  Podcast fundamentally changed the way I view AI safety/security. You have to listen to it. In general, the Dwarkesh Podcast is my favorite podcast besides Critical Thinking (of which I'm now the co-host).\nhttps://t.co/RdqNa9dElm\n---\nBest Impactful Security Write-Up\nHacking Kia by @samwcyo  and friends is super impactful continued research in their car-hacking saga.\nhttps://t.co/DOjiKVT6On\n---\nBest Command-Line AI Tool\nFabric - fantastic for piping content into LLMs from the command line.\nhttps://t.co/mrci2Barc2\n---\nBest Bug Bounty Program\n@GoogleVRP  - I spent the most time on it this year, and I got to go to Spain for their live hacking event.\nhttps://t.co/HWhgHW0tsr\n---\nBest AI Jailbreaking Resource\nL1B3RT4S - a collection of jailbreaks from @elder_plinius . I use this all the time.\nhttps://t.co/OzUqLIdlcS\n---\nBest Cybersecurity Model\nWhite Rabbit NEO - it's still got some work to do to get better, but I love the idea of a hacking-specific model that is pre-jailbroken so I can ask without getting rejections.\nhttps://t.co/htk7WQ4uvl\n---\nBest AI Engineering Book\nAI Engineering - this is basically the only book at this point, but I've heard great things about it.\nhttps://t.co/9t3ECVhLtP\n---\nBest AI YouTube Channel\n@MatthewBerman  Channel - He does chase the news a bit, but he works hard, is down-to-earth, and keeps improving. I enjoy his content.\nhttps://t.co/nhdjBUEOUQ\n---\nBest Prompting Guide\nEverything I Know About Prompting LLMs by @hrishioa  - my favorite guide to prompting.\nhttps://t.co/kFDNGLGdkr\n---\nBest Transcription App\nOffmute - using multimodal LLMs as STT was an idea i had and @hrishioa  nailed the execution here.\nhttps://t.co/8ViWEaEhPz\n\nTo get content like this, sign up to my email list: https://t.co/GSqFOAvtii"
    },
    "text": "I sent this to my email list and people loved it, so I wanted to post it here. It's a completely subjective list of stuff I loved in 2024. I hope you love it too😊 \n\nBest MacOS Desktop App\nmsty - fantastic AI client that supports local and API usage of models. You can use… https://t.co/mmOTUG2UJi https://t.co/tgZi36FfBs",
    "edit_history_tweet_ids": ["1881368754514231540"],
    "created_at": "2025-01-20T15:50:39.000Z",
    "author_id": "260411518"
  },
  {
    "id": "1881368466055381082",
    "text": "RT @xiangyue96: Part of my views on measuring AI performance were covered by Nature article recently. Defining and measuring intelligence i…",
    "edit_history_tweet_ids": ["1881368466055381082"],
    "created_at": "2025-01-20T15:49:31.000Z",
    "referenced_tweets": [
      {
        "type": "retweeted",
        "id": "1881364671816507432"
      }
    ],
    "author_id": "185910194"
  },
  {
    "id": "1881367237929615631",
    "text": "now available in ai-gradio\n\npip install --upgrade \"ai-gradio[deepseek]\"\n\nimport gradio as gr\nimport ai_gradio\n\ngr.load(\nname='deepseek:deepseek-reasoner',\nsrc=ai_gradio.registry,\ncoder=true\n).launch()\n\ngithub: https://t.co/vy68ARTizv",
    "edit_history_tweet_ids": ["1881367237929615631"],
    "in_reply_to_user_id": "2465283662",
    "created_at": "2025-01-20T15:44:38.000Z",
    "referenced_tweets": [
      {
        "type": "replied_to",
        "id": "1881364292374802514"
      }
    ],
    "author_id": "2465283662"
  },
  {
    "id": "1881366760630452523",
    "text": "wow @angelusm0rt1s kept telling me that Moonshot is an important lab and then they drop this. The paper seems legit. Also it's multimodal!\n\nI see no details about the model though. https://t.co/d7zV2p6FYx",
    "edit_history_tweet_ids": ["1881366760630452523"],
    "created_at": "2025-01-20T15:42:44.000Z",
    "referenced_tweets": [
      {
        "type": "quoted",
        "id": "1881332472748851259"
      }
    ],
    "author_id": "192201556"
  },
  {
    "id": "1881366569152049504",
    "text": "To celebrate this milestone, I'm working hard on an update to the Mamba guide (with animations!) and there's also a video version almost ready to record!\n\nLink to the newsletter:\nhttps://t.co/5dehBmx0XT",
    "edit_history_tweet_ids": ["1881366569152049504"],
    "in_reply_to_user_id": "1260142508748804096",
    "created_at": "2025-01-20T15:41:58.000Z",
    "referenced_tweets": [
      {
        "type": "replied_to",
        "id": "1881366561145200845"
      }
    ],
    "author_id": "1260142508748804096"
  },
  {
    "id": "1881366561145200845",
    "attachments": {
      "media_keys": ["3_1881366402642087936"]
    },
    "text": "I'm incredibly grateful that \"Exploring Language Models\" has reached over 10k subscribers🔥\n\nA big thank you to all readers who have enjoyed my visual guides to Mixture of Experts (MoE), Quantization, State Space Models (SSMs) and Mamba 🤗 https://t.co/j4GB6abawP",
    "edit_history_tweet_ids": ["1881366561145200845"],
    "created_at": "2025-01-20T15:41:56.000Z",
    "author_id": "1260142508748804096"
  },
  {
    "id": "1881363399780544671",
    "text": "Awesome new open source AI models released!\n\n@deepseek_ai leapfrogs OpenAI o1 (Deepseek-R1) but also distills that ability into smaller models (DeepSeek-R1-Distill) we can run on consumer hardware.\n\nAnd @MiniMax__AI has created a very competitive multimodal model (MiniMax-VL-01,… https://t.co/GHbIWXF3ZV",
    "edit_history_tweet_ids": ["1881363399780544671"],
    "created_at": "2025-01-20T15:29:23.000Z",
    "author_id": "2854214132",
    "note_tweet": {
      "text": "Awesome new open source AI models released!\n\n@deepseek_ai leapfrogs OpenAI o1 (Deepseek-R1) but also distills that ability into smaller models (DeepSeek-R1-Distill) we can run on consumer hardware.\n\nAnd @MiniMax__AI has created a very competitive multimodal model (MiniMax-VL-01, MiniMax-Text-01), with up to 4 million token context.\n\nVery excited to see how these innovations shake things up.  Thank you for your amazing contributions to open source AI!",
      "entities": {
        "mentions": [
          {
            "start": 45,
            "end": 57,
            "username": "deepseek_ai",
            "id": "1714580962569588736"
          },
          {
            "start": 203,
            "end": 215,
            "username": "MiniMax__AI",
            "id": "1875078099538423808"
          }
        ]
      }
    }
  },
  {
    "id": "1881363373251760291",
    "text": "RT @casper_hansen_: R1 supported, AGI unlocked?\n🚀AutoAWQ v0.2.8 is out!\n✨Major fixes &amp; now supports DeepSeek V3 + R1\n💾Compress your models…",
    "edit_history_tweet_ids": ["1881363373251760291"],
    "created_at": "2025-01-20T15:29:16.000Z",
    "referenced_tweets": [
      {
        "type": "retweeted",
        "id": "1881298724779094497"
      }
    ],
    "author_id": "2854214132"
  },
  {
    "id": "1881363356982153541",
    "attachments": {
      "media_keys": ["3_1881362538161405954"]
    },
    "text": "RL and open-source for the win!\n\nThe DeepSeek-R1 is a beast though. It still relies on multi-stage training and cold-start data before RL. \n\n\"DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks\" https://t.co/Ge42ZFq67D https://t.co/yHgdDN4M1t",
    "edit_history_tweet_ids": ["1881363356982153541"],
    "created_at": "2025-01-20T15:29:13.000Z",
    "referenced_tweets": [
      {
        "type": "quoted",
        "id": "1881318130334814301"
      }
    ],
    "author_id": "3448284313"
  },
  {
    "id": "1881363210982564304",
    "attachments": {
      "media_keys": ["3_1881363032514691072"]
    },
    "text": "Full snippet from the paper for those who are interested. I find this very compelling. We can just train specialized, Alpaca-style reasoning models relatively easily on our domain of choice.  And, adopting the more complex (RL-based) strategy for doing this isn't beneficial. https://t.co/DimGASsgOF",
    "edit_history_tweet_ids": ["1881363210982564304"],
    "in_reply_to_user_id": "1425585940542763010",
    "created_at": "2025-01-20T15:28:38.000Z",
    "referenced_tweets": [
      {
        "type": "replied_to",
        "id": "1881362098141446598"
      }
    ],
    "author_id": "1425585940542763010"
  },
  {
    "id": "1881362098141446598",
    "text": "Seems like RL finetuning small models is definitively worse than distilling the capabilities of a much larger RL finetuned model. We truly are about to enter the Alpaca era of reasoning models. May the best CoT win! https://t.co/mHeMcozPzD",
    "edit_history_tweet_ids": ["1881362098141446598"],
    "created_at": "2025-01-20T15:24:12.000Z",
    "referenced_tweets": [
      {
        "type": "quoted",
        "id": "1881318130334814301"
      }
    ],
    "author_id": "1425585940542763010"
  },
  {
    "id": "1881360794019156362",
    "attachments": {
      "media_keys": ["3_1881360790470696961"]
    },
    "text": "Agentic RAG Overview\n\nThis is a great intro to LLM agents and Agentic RAG.\n\nIt provides a comprehensive exploration of Agentic RAG architectures, applications, and implementation strategies. https://t.co/rPIXwp3hiL",
    "edit_history_tweet_ids": ["1881360794019156362"],
    "created_at": "2025-01-20T15:19:01.000Z",
    "author_id": "3448284313"
  },
  {
    "id": "1881357168814838064",
    "text": ".@DeepSeek_ai showed how pure reinforcement learning (RL) can improve models' reasoning 👇\n\n• They trained a base DeepSeek-V3 model using an RL framework called GRPO to get an improved model, DeepSeek-R1-Zero.\n\n• Building on the success of DeepSeek-R1-Zero, researchers… https://t.co/H5s25oyh9y https://t.co/vMhHvV1uZF",
    "edit_history_tweet_ids": ["1881357168814838064"],
    "created_at": "2025-01-20T15:04:37.000Z",
    "referenced_tweets": [
      {
        "type": "quoted",
        "id": "1881318130334814301"
      }
    ],
    "author_id": "1271482878958940160",
    "note_tweet": {
      "text": ".@DeepSeek_ai showed how pure reinforcement learning (RL) can improve models' reasoning 👇\n\n• They trained a base DeepSeek-V3 model using an RL framework called GRPO to get an improved model, DeepSeek-R1-Zero.\n\n• Building on the success of DeepSeek-R1-Zero, researchers developed the top-performing DeepSeek-R1. \n\nThey added several other stages:\n\n1. Cold start training: \nTraining with high-quality data makes the process easier and improves model's outputs.\n\n2. The first RL stage:\nDeepSeek combined reasoning accuracy and language consistency into a single score to guide the training.\n\n3. They fine-tuned the model using 800k samples (600k reasoning and 200k non-reasoning) to improve its overall capabilities.\n\n- The final RL stage refines the model to balance reasoning, helpfulness, and safety.\n\nAnd so, DeepSeek-R1 achieves this top level performance, especially in math, code, and reasoning tasks👇\n\nResearchers also transferred DeepSeek-R1's reasoning to smaller models like Qwen and Llama for better accessibility.",
      "entities": {
        "mentions": [
          {
            "start": 1,
            "end": 13,
            "username": "DeepSeek_ai",
            "id": "1714580962569588736"
          }
        ]
      }
    }
  },
  {
    "id": "1881353126210687089",
    "attachments": {
      "media_keys": ["3_1881345246191398912"]
    },
    "text": "We are living in a timeline where a non-US company is keeping the original mission of OpenAI alive - truly open, frontier research that empowers all. It makes no sense. The most entertaining outcome is the most likely.\n\nDeepSeek-R1 not only open-sources a barrage of models but… https://t.co/lEwLMTWprV https://t.co/M7eZnEmCOY",
    "edit_history_tweet_ids": ["1881353126210687089"],
    "created_at": "2025-01-20T14:48:33.000Z",
    "author_id": "1007413134",
    "note_tweet": {
      "text": "We are living in a timeline where a non-US company is keeping the original mission of OpenAI alive - truly open, frontier research that empowers all. It makes no sense. The most entertaining outcome is the most likely.\n\nDeepSeek-R1 not only open-sources a barrage of models but also spills all the training secrets. They are perhaps the first OSS project that shows major, sustained growth of an RL flywheel.\n\nImpact can be done by \"ASI achieved internally\" or mythical names like \"Project Strawberry\". \nImpact can also be done by simply dumping the raw algorithms and matplotlib learning curves.\n\nI'm reading the paper:\n\n> Purely driven by RL, no SFT at all (\"cold start\"). Reminiscent of AlphaZero - master Go, Shogi, and Chess from scratch, without imitating human grandmaster moves first. This is the most significant takeaway from the paper.\n> Use groundtruth rewards computed by hardcoded rules. Avoid any learned reward models that RL can easily hack against.\n> Thinking time of the model steadily increases as training proceeds - this is not pre-programmed, but an emergent property! \n> Emergence of self-reflection and exploration behaviors.\n> GRPO instead of PPO: it removes the critic net from PPO and uses the average reward of multiple samples instead. Simple method to reduce memory use. Note that GRPO was also invented by DeepSeek in Feb 2024 ... what a cracked team."
    }
  },
  {
    "id": "1881352022282383791",
    "attachments": {
      "media_keys": ["3_1881351925003624448"]
    },
    "text": "DeepSeek-R1 is here!  \n\nPerformance on par with OpenAI-o1, MIT licensed, and now available in ai-gradio\n\npip install --upgrade \"ai-gradio[deepseek]\n\nimport gradio as gr\nimport ai_gradio\n\ngr.load(\nname='deepseek:deepseek-reasoner',\nsrc=ai_gradio.registry,\n).launch() https://t.co/yOzi0lqKtC",
    "edit_history_tweet_ids": ["1881352022282383791"],
    "created_at": "2025-01-20T14:44:10.000Z",
    "author_id": "2465283662"
  },
  {
    "id": "1881351047312191960",
    "attachments": {
      "media_keys": ["7_1881349822793261060"]
    },
    "text": "I updated my repo, DeepSeek Engineer, to support the new reasoning model, deepseek-reasoner.\n\nThe script now:  \n- Shows the reasoning process before providing the final answer  \n- Maintains all original features (file operations, diff editing, etc.)  \n\nIt's pretty powerful! ⚡️ https://t.co/i27RQ3yxQP",
    "edit_history_tweet_ids": ["1881351047312191960"],
    "created_at": "2025-01-20T14:40:18.000Z",
    "author_id": "73105934"
  },
  {
    "id": "1881348370238951447",
    "text": "Wow, incredible work. Opensource model matching o1 on math/code benchmarks is wild, as is cheap distilled models and zero SFT/pure RL artifacts. \n\nSo how do you scale this to o3? Generate more synthetic data, verify, improve reward model, repeat? Seems doable... https://t.co/rtWo0ZqCTF",
    "edit_history_tweet_ids": ["1881348370238951447"],
    "created_at": "2025-01-20T14:29:39.000Z",
    "referenced_tweets": [
      {
        "type": "quoted",
        "id": "1881318130334814301"
      }
    ],
    "author_id": "811297774524317697"
  },
  {
    "id": "1881345378928427385",
    "attachments": {
      "media_keys": ["3_1881345008957083648"]
    },
    "text": "Distilling R1 onto Qwen 14B beats what the Qwen team did with the QwQ 32B model \n\nThis isn't talked about enough https://t.co/EtRqRzDSDa",
    "edit_history_tweet_ids": ["1881345378928427385"],
    "created_at": "2025-01-20T14:17:46.000Z",
    "author_id": "1718879852827484160"
  },
  {
    "id": "1881344893513297985",
    "attachments": {
      "media_keys": ["3_1881344346961649664"]
    },
    "text": "one more time for those in the back:\nQwen-14B SFT'd on R1 *strictly outperforms* deepseek-r1-lite-preview that used to be served on their web page\nI no longer care what that model was (tbh I do but they're not telling me), 16B, 27B, this is enough, thank you blessed whale https://t.co/NeMYYklEx5",
    "edit_history_tweet_ids": ["1881344893513297985"],
    "created_at": "2025-01-20T14:15:50.000Z",
    "author_id": "192201556"
  },
  {
    "id": "1881344482316374286",
    "text": "RT @multimodalart: ComfyUI → @huggingface Spaces → serverless ZeroGPU ✨😌\n\nWe wrote a tutorial on how to turn any ComfyUI workflow into an e…",
    "edit_history_tweet_ids": ["1881344482316374286"],
    "created_at": "2025-01-20T14:14:12.000Z",
    "referenced_tweets": [
      {
        "type": "retweeted",
        "id": "1879200050439668177"
      }
    ],
    "author_id": "1415428329210105859"
  },
  {
    "referenced_tweets": [
      {
        "type": "quoted",
        "id": "1881318135850213834"
      }
    ],
    "edit_history_tweet_ids": ["1881342078506139881"],
    "text": "This was posted in September (on codeforces)… now there is a 32B model distilled from r1 scoring 1600+ that you can run at home… wow https://t.co/LmKqbENI2y https://t.co/d6Vabn3sZJ",
    "author_id": "70514287",
    "attachments": {
      "media_keys": ["3_1881342075460825088"]
    },
    "id": "1881342078506139881",
    "created_at": "2025-01-20T14:04:39.000Z"
  },
  {
    "referenced_tweets": [
      {
        "type": "quoted",
        "id": "1881241250789265855"
      }
    ],
    "edit_history_tweet_ids": ["1881340583358161148"],
    "text": "Evaluations from the @deepseek_ai paper 💥\n\nDeepSeek-R1-Zero vs. OpenAI-o1-0912:\n\n&gt; R1-Zero achieves 71.0% pass@1 on AIME 2024, comparable to OpenAI-o1-0912 (74.4%)\n&gt; With majority voting, DeepSeek-R1-Zero reaches 86.7%, surpassing OpenAI-o1-0912\n\nDeepSeek-R1 vs. OpenAI-o1-1217:… https://t.co/oDleCqhyqK https://t.co/HLE4DND0Mi",
    "author_id": "874987512850128897",
    "note_tweet": {
      "text": "Evaluations from the @deepseek_ai paper 💥\n\nDeepSeek-R1-Zero vs. OpenAI-o1-0912:\n\n> R1-Zero achieves 71.0% pass@1 on AIME 2024, comparable to OpenAI-o1-0912 (74.4%)\n> With majority voting, DeepSeek-R1-Zero reaches 86.7%, surpassing OpenAI-o1-0912\n\nDeepSeek-R1 vs. OpenAI-o1-1217:\n\n> R1 achieves 79.8% pass@1 on AIME 2024, slightly surpassing OpenAI-o1-1217 (79.2%)\n> On MATH-500, DeepSeek-R1 scores 97.3%, matching OpenAI-o1-1217\n\n> On coding tasks, DeepSeek-R1 achieves a 2029 Elo rating on Codeforces, outperforming 96.3% of human participants\n\nDistilled Models:\n\n> R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview\n> R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench",
      "entities": {
        "mentions": [
          {
            "start": 21,
            "end": 33,
            "username": "deepseek_ai",
            "id": "1714580962569588736"
          }
        ]
      }
    },
    "id": "1881340583358161148",
    "created_at": "2025-01-20T13:58:43.000Z"
  },
  {
    "edit_history_tweet_ids": ["1881339724545286257"],
    "text": "China releasing a MIT license model that is on par with o1 and 30x cheaper was not on my bingo card",
    "author_id": "70514287",
    "id": "1881339724545286257",
    "created_at": "2025-01-20T13:55:18.000Z"
  },
  {
    "referenced_tweets": [
      {
        "type": "retweeted",
        "id": "1881334910528507989"
      }
    ],
    "edit_history_tweet_ids": ["1881336152952918463"],
    "text": "RT @TheXeophon: Notes:\n- Two models, R1-Zero (V3-Base + RL, no SFT), R1 (SFT [CoT from R1-Zero] -&gt; RL [reasoning] -&gt; SFT [general] -&gt; RL [a…",
    "author_id": "1499415401763115019",
    "id": "1881336152952918463",
    "created_at": "2025-01-20T13:41:07.000Z"
  },
  {
    "referenced_tweets": [
      {
        "type": "retweeted",
        "id": "1881333024358539615"
      }
    ],
    "edit_history_tweet_ids": ["1881333674563846311"],
    "text": "RT @sudu_cb: playjump image is LIVE for everyone\n\nCreate stunning images with FLUX from @bfl_ml \n・use Flux ultra, pro, dev and fast\n・blazin…",
    "author_id": "834956706",
    "id": "1881333674563846311",
    "created_at": "2025-01-20T13:31:16.000Z"
  },
  {
    "referenced_tweets": [
      {
        "type": "quoted",
        "id": "1859302712803807696"
      }
    ],
    "edit_history_tweet_ids": ["1881333351459717123", "1881333430472020099"],
    "text": "I don't think they have 50K Hopper GPUs, as they are still using GPU-poor methods like GRPO. \nWhat they might have is 20K IQ points total across their ≈140 researchers, however. Give or take.\n\nMaybe that's more important. https://t.co/9S9x3vWU5b",
    "author_id": "192201556",
    "id": "1881333430472020099",
    "created_at": "2025-01-20T13:30:17.000Z"
  },
  {
    "referenced_tweets": [
      {
        "type": "replied_to",
        "id": "1881330757001502991"
      }
    ],
    "edit_history_tweet_ids": ["1881331142688989657"],
    "text": "my assumption always was that for distillation, sft reasoning traces would help with CoT formatting, and then get better results with RL on top, i didn't think it would help teach models which traces are correct and make it learn proper CoT reasoning",
    "author_id": "1499415401763115019",
    "in_reply_to_user_id": "1499415401763115019",
    "id": "1881331142688989657",
    "created_at": "2025-01-20T13:21:12.000Z"
  },
  {
    "edit_history_tweet_ids": ["1881330794549182853"],
    "text": "This figure right here is the single biggest reason why we will never get o1/o3's reasoning traces.\n\nThe unreasonable effectiveness of distilling from a reasoner https://t.co/MwhBWQnInJ",
    "author_id": "1718879852827484160",
    "attachments": {
      "media_keys": ["3_1881330681336209408"]
    },
    "id": "1881330794549182853",
    "created_at": "2025-01-20T13:19:49.000Z"
  },
  {
    "edit_history_tweet_ids": ["1881330757001502991"],
    "text": "this is the most surprising thing from the r1 paper to me:\n&gt;reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models.\n\ntotal sft victory https://t.co/xPwOAqnFTB https://t.co/rWSzHoLlZI",
    "author_id": "1499415401763115019",
    "note_tweet": {
      "text": "this is the most surprising thing from the r1 paper to me:\n>reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models.\n\ntotal sft victory"
    },
    "attachments": {
      "media_keys": ["3_1881330548229988352"]
    },
    "id": "1881330757001502991",
    "created_at": "2025-01-20T13:19:40.000Z"
  },
  {
    "referenced_tweets": [
      {
        "type": "retweeted",
        "id": "1881288088481132855"
      }
    ],
    "edit_history_tweet_ids": ["1881330124857200998"],
    "text": "RT @uphillconf: 🎉 Excited to announce our first talk for #UphillConf2025:\n\n\"The Rise of Small Models: On-Device Language Models and SmolLM\"…",
    "author_id": "1188812448767336449",
    "id": "1881330124857200998",
    "created_at": "2025-01-20T13:17:09.000Z"
  },
  {
    "edit_history_tweet_ids": ["1881329993206374443"],
    "text": "Good morning. The biggest release of the past few months has arrived. \n\nThe first true o1 replication https://t.co/o9jqcyf5Qx",
    "author_id": "1718879852827484160",
    "attachments": {
      "media_keys": ["3_1881329316094521345"]
    },
    "id": "1881329993206374443",
    "created_at": "2025-01-20T13:16:38.000Z"
  },
  {
    "referenced_tweets": [
      {
        "type": "replied_to",
        "id": "1881318686189137929"
      }
    ],
    "edit_history_tweet_ids": ["1881329351125549144"],
    "text": "And another unexpected win for @doomslide and @_xjdr  \nthe initial expectations for OpenAI hidden CoT were that it's… not very human. that seems overblown but this is the first confirmation that naturally evolved CoTs can be weird. https://t.co/7IvaPjvuGm",
    "author_id": "192201556",
    "in_reply_to_user_id": "192201556",
    "attachments": {
      "media_keys": ["3_1881329003618836480"]
    },
    "id": "1881329351125549144",
    "created_at": "2025-01-20T13:14:05.000Z"
  },
  {
    "referenced_tweets": [
      {
        "type": "retweeted",
        "id": "1880728370440548798"
      }
    ],
    "edit_history_tweet_ids": ["1881328851898544583"],
    "text": "RT @kimmonismus: Nvidia's Jim Fan: We're training robots in a simulation that accelerates physics by 10,000x. The robots undergo 1 year of…",
    "author_id": "1007413134",
    "id": "1881328851898544583",
    "created_at": "2025-01-20T13:12:06.000Z"
  },
  {
    "edit_history_tweet_ids": ["1881326878772072818"],
    "text": "deepseek officially released a distillation of r1 trained on llama 8b, time to try it at home https://t.co/XBWMO7IKgf",
    "author_id": "1499415401763115019",
    "attachments": {
      "media_keys": ["3_1881326762631512064"]
    },
    "id": "1881326878772072818",
    "created_at": "2025-01-20T13:04:15.000Z"
  },
  {
    "referenced_tweets": [
      {
        "type": "retweeted",
        "id": "1881290084378648839"
      }
    ],
    "edit_history_tweet_ids": ["1881326097884946741"],
    "text": "RT @DataChaz: 35K ppl have already registered, and there's still time to sign up for the FREE @huggingface AI Agents Course! 🤗\n\nIn this cou…",
    "author_id": "186420551",
    "id": "1881326097884946741",
    "created_at": "2025-01-20T13:01:09.000Z"
  },
  {
    "referenced_tweets": [
      {
        "type": "quoted",
        "id": "1881318130334814301"
      }
    ],
    "edit_history_tweet_ids": ["1881322593065869582"],
    "text": "DeepSeek-R1:\n\n- Open weights\n- MIT license\n- API outputs can be used for finetuning.\n- Free to use on webpage and app.\n\nSalute to our R1 team for this amazing CNY delivery 🎁🫡🎉! https://t.co/74jYwKVHv8",
    "author_id": "1496348031494819842",
    "id": "1881322593065869582",
    "created_at": "2025-01-20T12:47:14.000Z"
  },
  {
    "referenced_tweets": [
      {
        "type": "quoted",
        "id": "1881318686189137929"
      }
    ],
    "edit_history_tweet_ids": ["1881322387091882491"],
    "text": "I have always argued that RL has to be worth it and that most RL people whined at me to do has **not been worth it** - and I don't apologize for it.\n\nThis RL, however, does seem worth it though, and we've been working on a system for reasoning RL that is pretty much exactly what… https://t.co/VCMCHs4kfF https://t.co/Q1mLpiTAyx",
    "author_id": "1365020011123773442",
    "note_tweet": {
      "text": "I have always argued that RL has to be worth it and that most RL people whined at me to do has **not been worth it** - and I don't apologize for it.\n\nThis RL, however, does seem worth it though, and we've been working on a system for reasoning RL that is pretty much exactly what they did in deepseek. I dont know how they move so fast but its a blessing, and so is their paper release. \n\nI believe this RL does have the potential to create self improving models. Or at least, self improve to some higher point then the first round does. I also believe this kind of RL has the potential to be the most interesting way to train a model, because it learns to think **on its own** by exploring which thoughts lead to the correct outcome. Seeing the CoT is as much fun and is as intriguing to me as getting the right answer\n\nSFT from distillation simply offers more bang for your buck. In our experience has been between 2 and 14x less costly than RL, and the existing RLHF work, whether RM's, DPO datasets, etc out there have not improved the models much - surprisingly, much less at large scales (because the existing datasets likely reach a basin around 20-30B param model average nuanced preferences or something). Look at our Hermes 3 technical report, where 8B did benefit from RLHF, but 70B and 405B did not.\n\nThe challenges of working across multiple base models and sizes also makes it difficult to maintain what some people like Roon claim is basically a golden rule of RL, on policy data. Requiring new RM or DPO datasets for every damn model we want to improve. The overhead is large, so the benefit must be proportionally large!\n\nAnyone who is worried we aren't taking what actually effectively improves the model in an efficient way seriously does not need to worry.\n\nThe post training work continues!"
    },
    "id": "1881322387091882491",
    "created_at": "2025-01-20T12:46:25.000Z"
  },
  {
    "edit_history_tweet_ids": ["1881320402640928837"],
    "text": "A 1.5B model achieving 28% on AIME, 33.8% on GPQAm 16.9% on LiveCodeBench and 954 on CodeForces.🤯\n\nhttps://t.co/PlwIxHM5bb",
    "author_id": "1141052916570214400",
    "id": "1881320402640928837",
    "created_at": "2025-01-20T12:38:31.000Z"
  },
  {
    "edit_history_tweet_ids": ["1881319751680733238"],
    "text": "The new distilled @deepseek_ai models are fine-tuned versions of @Alibaba_Qwen or @AIatMeta Llama 3 using  800k samples curated with DeepSeek-R1. (no RL stage) https://t.co/2ytEgzZIvv",
    "author_id": "1141052916570214400",
    "attachments": {
      "media_keys": ["3_1881319663168077824"]
    },
    "id": "1881319751680733238",
    "created_at": "2025-01-20T12:35:56.000Z"
  },
  {
    "edit_history_tweet_ids": ["1881319500089634954"],
    "text": "\"DeepSeek-R1-Distill-Qwen-1.5B outperforms GPT-4o and Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH.\"\n\n1.5B did WHAT? https://t.co/Pk6fOJNma2",
    "author_id": "874987512850128897",
    "attachments": {
      "media_keys": ["3_1881319353355755520"]
    },
    "id": "1881319500089634954",
    "created_at": "2025-01-20T12:34:56.000Z"
  },
  {
    "referenced_tweets": [
      {
        "type": "quoted",
        "id": "1881318130334814301"
      }
    ],
    "edit_history_tweet_ids": ["1881318293384102130"],
    "text": "Here comes DeepSeek-R1, our latest reasoning model with significantly enhanced reasoning abilities. We also share a technical report on how we train the reasoning model with large-scale RL. Have fun! https://t.co/BVFNwMZTUc",
    "author_id": "1187046037099147265",
    "id": "1881318293384102130",
    "created_at": "2025-01-20T12:30:09.000Z"
  },
  {
    "referenced_tweets": [
      {
        "type": "replied_to",
        "id": "1881318142083018951"
      }
    ],
    "edit_history_tweet_ids": ["1881318145761439995"],
    "text": "🌐 API Access &amp; Pricing\n\n⚙️ Use DeepSeek-R1 by setting model=deepseek-reasoner\n💰 $0.14 / million input tokens (cache hit)\n💰 $0.55 / million input tokens (cache miss)\n💰 $2.19 / million output tokens\n\n📖 API guide: https://t.co/Qf97ASptDD\n\n🐋 5/n https://t.co/v5ho1VOex5",
    "author_id": "1714580962569588736",
    "in_reply_to_user_id": "1714580962569588736",
    "attachments": {
      "media_keys": ["3_1881317215393468416", "3_1881317224910299137"]
    },
    "id": "1881318145761439995",
    "created_at": "2025-01-20T12:29:33.000Z"
  },
  {
    "referenced_tweets": [
      {
        "type": "replied_to",
        "id": "1881318138937233664"
      }
    ],
    "edit_history_tweet_ids": ["1881318142083018951"],
    "text": "🛠️ DeepSeek-R1: Technical Highlights\n\n📈 Large-scale RL in post-training\n🏆 Significant performance boost with minimal labeled data\n🔢 Math, code, and reasoning tasks on par with OpenAI-o1\n📄 More details: https://t.co/jWMxMVhGAQ\n\n🐋 4/n https://t.co/mIUBn3qJhQ",
    "author_id": "1714580962569588736",
    "in_reply_to_user_id": "1714580962569588736",
    "attachments": {
      "media_keys": ["3_1881317132170059776"]
    },
    "id": "1881318142083018951",
    "created_at": "2025-01-20T12:29:32.000Z"
  },
  {
    "referenced_tweets": [
      {
        "type": "replied_to",
        "id": "1881318130334814301"
      }
    ],
    "edit_history_tweet_ids": ["1881318135850213834"],
    "text": "🔥 Bonus: Open-Source Distilled Models!\n\n🔬 Distilled from DeepSeek-R1, 6 small models fully open-sourced\n📏 32B &amp; 70B models on par with OpenAI-o1-mini\n🤝 Empowering the open-source community\n\n🌍 Pushing the boundaries of **open AI**!\n\n🐋 2/n https://t.co/tfXLM2xtZZ",
    "author_id": "1714580962569588736",
    "in_reply_to_user_id": "1714580962569588736",
    "attachments": {
      "media_keys": ["3_1881316978297851904"]
    },
    "id": "1881318135850213834",
    "created_at": "2025-01-20T12:29:31.000Z"
  },
  {
    "edit_history_tweet_ids": ["1881318130334814301"],
    "text": "🚀 DeepSeek-R1 is here!\n\n⚡ Performance on par with OpenAI-o1\n📖 Fully open-source model &amp; technical report\n🏆 MIT licensed: Distill &amp; commercialize freely!\n\n🌐 Website &amp; API are live now! Try DeepThink at https://t.co/v1TFy7LHNy today!\n\n🐋 1/n https://t.co/7BlpWAPu6y",
    "author_id": "1714580962569588736",
    "attachments": {
      "media_keys": ["3_1881318116745150464"]
    },
    "id": "1881318130334814301",
    "created_at": "2025-01-20T12:29:30.000Z"
  },
  {
    "edit_history_tweet_ids": ["1881315419086291213"],
    "text": "holy fuck, these gigachads dropped 6 distilled models right from 1.5B to 70B 🔥 https://t.co/DYq0cobPiq",
    "author_id": "874987512850128897",
    "attachments": {
      "media_keys": ["3_1881315249187328001"]
    },
    "id": "1881315419086291213",
    "created_at": "2025-01-20T12:18:43.000Z"
  },
  {
    "id": "1881313255555305595",
    "text": "holy crap they actually got tired of people complaining that their models are impossible to run locally, and just did the whole lineup\n\nIt will be really funny if this Qwen distill outperforms QwQ-preview https://t.co/gxVzMcbeUu https://t.co/CIhtLJaTBU",
    "author_id": "192201556",
    "attachments": {
      "media_keys": ["3_1881312848384593920", "3_1881313021181448192"]
    },
    "created_at": "2025-01-20T12:10:07.000Z",
    "referenced_tweets": [
      {
        "type": "quoted",
        "id": "1881311174610051491"
      }
    ],
    "edit_history_tweet_ids": ["1881313255555305595"]
  },
  {
    "id": "1881303164336406582",
    "text": "Top stories in AI today:\n\n-OpenAI readies ‘o3-mini’ model launch\n-Altman to brief Washington on ‘PhD level SuperAgents’\n-Get automatic global news briefings with ChatGPT\n-Runway releases ‘Frames’ image generation model\n-4 new AI tools &amp; 4 job opportunities \n\nRead more:… https://t.co/nqfyjaE0pH https://t.co/PaXuZKTgkl",
    "note_tweet": {
      "text": "Top stories in AI today:\n\n-OpenAI readies ‘o3-mini’ model launch\n-Altman to brief Washington on ‘PhD level SuperAgents’\n-Get automatic global news briefings with ChatGPT\n-Runway releases ‘Frames’ image generation model\n-4 new AI tools & 4 job opportunities \n\nRead more: https://t.co/yMkAMCRwFQ",
      "entities": {
        "urls": [
          {
            "start": 270,
            "end": 293,
            "url": "https://t.co/yMkAMCRwFQ",
            "expanded_url": "http://therundown.ai/p/openais-o3-mini-is-coming",
            "display_url": "therundown.ai/p/openais-o3-m…"
          }
        ]
      }
    },
    "author_id": "731917653506318336",
    "attachments": {
      "media_keys": ["3_1881303161614237696"]
    },
    "created_at": "2025-01-20T11:30:01.000Z",
    "edit_history_tweet_ids": ["1881303164336406582"]
  },
  {
    "id": "1881298065967239183",
    "text": "As VL2-27B LLM part is similar to V3, I guess that's the only \"lite\" they used to prep for R1.\nThere's never been a V2-based R1. I've been underestimating the Whale. They don't have compute to waste on half-measures.\nV3-R1 began training less than 1 month after o1 announcement. https://t.co/LTukt2AYby",
    "author_id": "192201556",
    "attachments": {
      "media_keys": ["3_1881297822076563456", "3_1881297929740115968"]
    },
    "in_reply_to_user_id": "192201556",
    "created_at": "2025-01-20T11:09:46.000Z",
    "referenced_tweets": [
      {
        "type": "replied_to",
        "id": "1881295618192077099"
      }
    ],
    "edit_history_tweet_ids": ["1881298065967239183"]
  },
  {
    "id": "1881295618192077099",
    "text": "I conclude that R1, and by implication V3-base, were completed before December 10. It seems that V2.5-1210 is this ablation experiment in V3 paper (check LCB and MATH).\nWhale never trains models as mere products. Every single training run is a step in research towards AGI. https://t.co/o7VKGkQwap https://t.co/dDGrHyZZfi https://t.co/S9fZGZylCZ",
    "note_tweet": {
      "text": "I conclude that R1, and by implication V3-base, were completed before December 10. It seems that V2.5-1210 is this ablation experiment in V3 paper (check LCB and MATH).\nWhale never trains models as mere products. Every single training run is a step in research towards AGI."
    },
    "author_id": "192201556",
    "attachments": {
      "media_keys": ["3_1881294158226235393", "3_1881294750055084032"]
    },
    "created_at": "2025-01-20T11:00:02.000Z",
    "referenced_tweets": [
      {
        "type": "quoted",
        "id": "1866426008108200244"
      }
    ],
    "edit_history_tweet_ids": ["1881295618192077099"]
  },
  {
    "id": "1881293981700829415",
    "text": "deepseek is just so based, casually stealth drops the most powerful oss model ever,",
    "author_id": "1499415401763115019",
    "created_at": "2025-01-20T10:53:32.000Z",
    "edit_history_tweet_ids": ["1881293981700829415"]
  },
  {
    "id": "1881277875502157873",
    "text": "RT @LiquidAI_: Introducing LFM-7B, our new best-in-class language model in English, Arabic, and Japanese optimized to be the substrate for…",
    "author_id": "923114064460558336",
    "created_at": "2025-01-20T09:49:32.000Z",
    "referenced_tweets": [
      {
        "type": "retweeted",
        "id": "1881236162893000944"
      }
    ],
    "edit_history_tweet_ids": ["1881277875502157873"]
  },
  {
    "id": "1881275086394843449",
    "text": "@LiquidAI_ You can play with it on Liquid's Playground: https://t.co/890Zhdr5h6\n\nHere's the full blog post: https://t.co/ZNWTwS9YDF https://t.co/wEmgNO5kgW",
    "author_id": "923114064460558336",
    "attachments": {
      "media_keys": ["3_1881275077372596224"]
    },
    "in_reply_to_user_id": "923114064460558336",
    "created_at": "2025-01-20T09:38:27.000Z",
    "referenced_tweets": [
      {
        "type": "replied_to",
        "id": "1881274480821215246"
      }
    ],
    "edit_history_tweet_ids": ["1881275086394843449"]
  },
  {
    "id": "1881274480821215246",
    "text": "@LiquidAI_ Thanks to the LFM architecture, it also benefits from fast inference and much lower memory usage compared to other 7-8B models.\n\nIt delivers an effective 32k context window with top scores on LongBench v2 (32k without CoT). https://t.co/zy2BB4XP2h",
    "author_id": "923114064460558336",
    "attachments": {
      "media_keys": ["3_1881274461246107648"]
    },
    "in_reply_to_user_id": "923114064460558336",
    "created_at": "2025-01-20T09:36:03.000Z",
    "referenced_tweets": [
      {
        "type": "replied_to",
        "id": "1881274341385789830"
      }
    ],
    "edit_history_tweet_ids": ["1881274480821215246"]
  },
  {
    "id": "1881274341385789830",
    "text": "@LiquidAI_ This was made possible by combining supervised fine-tuning, preference alignment, and model merging with extremely high-quality data and comprehensive evaluations (the whole post-training loop). https://t.co/Ut7EnTk3IH",
    "author_id": "923114064460558336",
    "attachments": {
      "media_keys": ["3_1881274321147994112"]
    },
    "in_reply_to_user_id": "923114064460558336",
    "created_at": "2025-01-20T09:35:30.000Z",
    "referenced_tweets": [
      {
        "type": "replied_to",
        "id": "1881274092994933033"
      }
    ],
    "edit_history_tweet_ids": ["1881274341385789830"]
  },
  {
    "id": "1881274092994933033",
    "text": "🥳 Super happy to share the new model I've been working on: LFM-7B\n\nLFM-7B was designed for exceptional multilingual chat capabilities, including Arabic and Japanese.\n\nIt's by far our best post-training piece to date at @LiquidAI_. https://t.co/jfAXb5FHDw",
    "author_id": "923114064460558336",
    "attachments": {
      "media_keys": ["3_1881273904292937728"]
    },
    "created_at": "2025-01-20T09:34:30.000Z",
    "edit_history_tweet_ids": ["1881274092994933033"]
  },
  {
    "id": "1881267954769105015",
    "text": "RT @adcock_brett: A ton of great progress in AI and Robotics this week.\n\nI summarized everything from Unitree, OpenAI, Mirror Me, Microsoft…",
    "author_id": "3222018178",
    "created_at": "2025-01-20T09:10:07.000Z",
    "referenced_tweets": [
      {
        "type": "retweeted",
        "id": "1881024584515830239"
      }
    ],
    "edit_history_tweet_ids": ["1881267954769105015"]
  },
  {
    "id": "1881258625999786422",
    "text": "New inference-time computing method from @GoogleDeepMind! Evolutionary search strategy focuses on improving planning by generating multiple solutions in parallel, evaluating and recombining them.\n\nImplementation\n0️⃣ planning problems and a programmatic way to evaluate the quality… https://t.co/9Nnmg7h2cf https://t.co/17ecEsoZE5",
    "note_tweet": {
      "text": "New inference-time computing method from @GoogleDeepMind! Evolutionary search strategy focuses on improving planning by generating multiple solutions in parallel, evaluating and recombining them.\n\nImplementation\n0️⃣ planning problems and a programmatic way to evaluate the quality of solutions. \n1️⃣ Use an LLM (e.g. Gemini) to generate an initial set of diverse candidate solutions. \n2️⃣ Evaluate each solution using the evaluator and select the best-performing ones \n3️⃣ Use the LLM to create new solutions by combining elements from the best solutions \n4️⃣ Have the LLM refine these new solutions based on evaluator feedback \n5️⃣ Repeat steps 3-5 for multiple generations until you find a satisfactory solution\n\nCan be used in a two-stage approach: start with a faster model (Gemini Flash) and use a more powerful model (like Gemini Pro) for difficult cases\n\nInsights\n🚀 98% success rate on TravelPlanner and Natural Plan benchmarks using Gemini 1.5 Pro.\n💪 Outperforms Best-of-N and Sequential Revision strategies significantly.\n💰 Mind Evolution more cost-effective than sequential revision ($0.65 vs $3.20 per problem)\n🔄 Deeper search (more generations) performed better than generating more candidates\n🎯 Works well for problems that are hard to formalize but easy to evaluate\n🥇 Achieved 46.5% success rate in the StegPoet compared to 1% for Best-of-N\n📝 Prompts are part of the paper and include general instructions, problem definition, few-shot examples\n❌ Requires a programmatic evaluator that can score solutions and provide feedback",
      "entities": {
        "mentions": [
          {
            "start": 41,
            "end": 56,
            "username": "GoogleDeepMind",
            "id": "4783690002"
          }
        ]
      }
    },
    "author_id": "1141052916570214400",
    "attachments": {
      "media_keys": ["3_1881258572144672768"]
    },
    "created_at": "2025-01-20T08:33:03.000Z",
    "edit_history_tweet_ids": ["1881258625999786422"]
  },
  {
    "id": "1881258443669172470",
    "text": "twitter hype is out of control again. \n\nwe are not gonna deploy AGI next month, nor have we built it.\n\nwe have some very cool stuff for you but pls chill and cut your expectations 100x!",
    "author_id": "1605",
    "created_at": "2025-01-20T08:32:19.000Z",
    "edit_history_tweet_ids": ["1881258443669172470"]
  },
  {
    "id": "1881252048584524133",
    "text": "Now that we know R1 uses v3 as a base, worth remembering that v3 was released less than a month ago\n\nEspecially important if you believe the rumor that v3 only finished training just before the release",
    "author_id": "1718879852827484160",
    "created_at": "2025-01-20T08:06:55.000Z",
    "edit_history_tweet_ids": ["1881252048584524133"]
  },
  {
    "id": "1881250338893668464",
    "text": "🚨 Last Chance to RSVP! 🚨\n\nDon’t miss the Building Intelligent Agentic RAG live stream with @crewAIInc &amp; Qdrant!\n\nTomorrow, @LukawskiKacper and @tonykipkemboi will demonstrate how to create advanced AI agents that work with your emails and knowledge base to generate… https://t.co/mMksGsnnJm https://t.co/2cThiISnIk",
    "note_tweet": {
      "text": "🚨 Last Chance to RSVP! 🚨\n\nDon’t miss the Building Intelligent Agentic RAG live stream with @crewAIInc & Qdrant!\n\nTomorrow, @LukawskiKacper and @tonykipkemboi will demonstrate how to create advanced AI agents that work with your emails and knowledge base to generate context-driven responses and automate personal tasks. \n\n👉 Register now: https://t.co/MKud2hMTfv",
      "entities": {
        "mentions": [
          {
            "start": 91,
            "end": 101,
            "username": "crewAIInc",
            "id": "1770815821310230528"
          },
          {
            "start": 123,
            "end": 138,
            "username": "LukawskiKacper",
            "id": "4782705362"
          },
          {
            "start": 143,
            "end": 157,
            "username": "tonykipkemboi",
            "id": "850175781468819456"
          }
        ],
        "urls": [
          {
            "start": 338,
            "end": 361,
            "url": "https://t.co/MKud2hMTfv",
            "expanded_url": "https://buff.ly/4fQknd6",
            "display_url": "buff.ly/4fQknd6"
          }
        ]
      }
    },
    "author_id": "1338631899422617600",
    "attachments": {
      "media_keys": ["3_1881250336486170624"]
    },
    "created_at": "2025-01-20T08:00:07.000Z",
    "edit_history_tweet_ids": ["1881250338893668464"]
  },
  {
    "id": "1881250036803359214",
    "text": "DeepSeek R1 Zero is here. It looks to be from DeepSeek v3 which means that R1-Lite/Preview that we saw previously is a completely separate training run https://t.co/2ldP8uTUjA",
    "author_id": "1718879852827484160",
    "attachments": {
      "media_keys": ["3_1881249772322832384"]
    },
    "created_at": "2025-01-20T07:58:55.000Z",
    "edit_history_tweet_ids": ["1881250036803359214"]
  },
  {
    "id": "1881241867335176475",
    "text": "Curious detail: transformers_version in configs of both R1s is 4.46.3, which was released Nov 18, 2024. R1-Lite-Preview was debuted Nov 20, 2024.\n(V3's transformers_version is ancient, Sep 2023 4.33.1. Likewise for V2-Lite. V2-coders are 4.39.3 – from April 2024) https://t.co/GFY8aysA8r",
    "author_id": "192201556",
    "attachments": {
      "media_keys": ["3_1881239818295779328"]
    },
    "created_at": "2025-01-20T07:26:27.000Z",
    "edit_history_tweet_ids": ["1881241867335176475"]
  },
  {
    "id": "1881241099857174742",
    "text": "RT @RisingSayak: What up Bangalore? \n\nHF &amp; Google Cloud are organizing an in-person meetup on 25th Jan on multimodal models, featuring Pali…",
    "author_id": "593407733",
    "created_at": "2025-01-20T07:23:24.000Z",
    "referenced_tweets": [
      {
        "type": "retweeted",
        "id": "1879870116802048292"
      }
    ],
    "edit_history_tweet_ids": ["1881241099857174742"]
  },
  {
    "id": "1881241076679532590",
    "text": "RT @RisingSayak: Nice work from GDM on going beyond steps to scale test-time compute for diffusion models. \n\nI know this is not the first w…",
    "author_id": "593407733",
    "created_at": "2025-01-20T07:23:19.000Z",
    "referenced_tweets": [
      {
        "type": "retweeted",
        "id": "1880095232706113726"
      }
    ],
    "edit_history_tweet_ids": ["1881241076679532590"]
  },
  {
    "id": "1881238789995254028",
    "text": "RT @SonglinYang4: I've created slides for those curious about the recent rapid progress in linear attention: from linear attention to Light…",
    "author_id": "568879807",
    "created_at": "2025-01-20T07:14:13.000Z",
    "referenced_tweets": [
      {
        "type": "retweeted",
        "id": "1880536012432265349"
      }
    ],
    "edit_history_tweet_ids": ["1881238789995254028"]
  }
]

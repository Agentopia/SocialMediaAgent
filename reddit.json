[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "95r93m5fl98612cbd1ad040091006430602872d1c73c019305",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hey r/LocalLLaMA! I added 2x faster vision finetuning support in [Unsloth](https://github.com/unslothai/unsloth), but some people complained about 4bit quants not performing well. I did an investigation, and it looks like quantizing all layers to 4bit will sometimes break your model! I uploaded mixed 4bit and 16bit weights which aim to recover the accuracy fully.\n\nFor example using Qwen2-VL-2B Instruct, and given an image below:\n\nhttps://preview.redd.it/3liwqysetv4e1.png?width=640&amp;format=png&amp;auto=webp&amp;s=3640abecd874d96c89b0c68e7bdcd1f80cf9f76b\n\n|Quantization|Description|Size|Result|\n|:-|:-|:-|:-|\n|16bit|The image shows a train traveling on tracks.|4.11GB|\u2705|\n|Default 4bit all layers|The image depicts a vibrant and colorful scene of a coastal area.|1.36GB|\u274c Definitely wrong|\n|Unsloth quant|The image shows a train traveling on tracks.|1.81GB|\u2705|\n\nWe see 4bit on all layers breaks Qwen2-VL-2B Instruct. So the trick is to carefully select only some layers to quantize and leave 10% or so in full precision! The main issue is some layers have large outliers, and so we have to inspect both the activation errors (like AWQ) and also weight quantization errors (like HQQ / bitsandbytes). For example if you look at Llama 3.2 11B Vision Instruct's error analysis below:\n\nhttps://preview.redd.it/xi4zklxftv4e1.png?width=1000&amp;format=png&amp;auto=webp&amp;s=15cdba4212771030abe1e8b6ba8e4a36f44aeefd\n\nWe see that:\n\n* There is a large spike in activation error in a MLP layer.\n* There are large repeating spikes in weight quantization errors, and these correspond to the the Cross Attention layers.\n\nI uploaded all dynamic Unsloth quants below. I also attached free Colab Notebooks to finetune / do inference on vision models with Unsloth up to **2x faster and use up to 50% less VRAM**!\n\n|Model|Model Page|Colab Notebook|\n|:-|:-|:-|\n|Llama 3.2 11B Vision Instruct|[Dynamic quant](https://huggingface.co/unsloth/Llama-3.2-11B-Vision-Instruct-unsloth-bnb-4bit)|[Colab Notebook](https://colab.research.google.com/drive/1j0N4XTY1zXXy7mPAhOC1_gMYZ2F2EBlk?usp=sharing)|\n|Llama 3.2 11B Vision Base|[Dynamic quant](https://huggingface.co/unsloth/Llama-3.2-11B-Vision-unsloth-bnb-4bit)|Change model name in [Llama 11B Instruct Notebook](https://colab.research.google.com/drive/1j0N4XTY1zXXy7mPAhOC1_gMYZ2F2EBlk?usp=sharing)|\n|Qwen2 VL 2B Instruct|[Dynamic quant](https://huggingface.co/unsloth/Qwen2-VL-2B-Instruct-unsloth-bnb-4bit)|Change model name in [Qwen 7B Instruct Notebook](https://colab.research.google.com/drive/1whHb54GNZMrNxIsi2wm2EY_-Pvo2QyKh?usp=sharing)|\n|Qwen2 VL 7B Instruct|[Dynamic quant](https://huggingface.co/unsloth/Qwen2-VL-7B-Instruct-unsloth-bnb-4bit)|[Colab Notebook](https://colab.research.google.com/drive/1whHb54GNZMrNxIsi2wm2EY_-Pvo2QyKh?usp=sharing)|\n|Pixtral 12B Instruct|[Dynamic quant](https://huggingface.co/unsloth/Pixtral-12B-2409-unsloth-bnb-4bit)|[Colab Notebook](https://colab.research.google.com/drive/1K9ZrdwvZRE96qGkCq_e88FgV3MLnymQq?usp=sharing)|\n|QwQ 32B Preview|[Dynamic quant](https://huggingface.co/unsloth/QwQ-32B-Preview-unsloth-bnb-4bit)|Change model name in [Qwen 2.5 Coder Notebook](https://colab.research.google.com/drive/1qN1CEalC70EO1wGKhNxs1go1W9So61R5?usp=sharing)|\n\nI added more experiments and details in the blog post here: [https://unsloth.ai/blog/dynamic-4bit](https://unsloth.ai/blog/dynamic-4bit) . Also there are some bugs / issues which I fixed as well in Unsloth, so please update it!\n\n* Llama.cpp GGUF changed from `make` to `cmake` breaking saving\n* Finetuning then merging to 16bit broke - fixed this now!\n* V100s and older GPUs broke for finetuning - fixed as well!\n\nPlease update Unsloth via `pip install --upgrade --no-cache-dir --no-deps unsloth unsloth_zoo`! I also put free Colabs and Kaggle notebooks to finetune Llama, Mistral, Gemma, Phi, Qwen and more on the Github here: [https://github.com/unslothai/unsloth](https://github.com/unslothai/unsloth) and all model uploads are here: [https://huggingface.co/unsloth](https://huggingface.co/unsloth) . Thanks a lot and have a great day!",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Quantizing to 4bits can break models - Dynamic quantization 10% FP16 90% 4bit",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": 73,
            "top_awarded_type": null,
            "hide_score": false,
            "media_metadata": {
              "3liwqysetv4e1": {
                "status": "valid",
                "e": "Image",
                "m": "image/png",
                "p": [
                  {
                    "y": 72,
                    "x": 108,
                    "u": "https://preview.redd.it/3liwqysetv4e1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4d2075d598eb3b95bf735d738047537cb420d58c"
                  },
                  {
                    "y": 144,
                    "x": 216,
                    "u": "https://preview.redd.it/3liwqysetv4e1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=11d2b229df6eb9eb2c5c5234c79342fdfbc0163b"
                  },
                  {
                    "y": 213,
                    "x": 320,
                    "u": "https://preview.redd.it/3liwqysetv4e1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=700a122d17ee4cb8394c2428fb57da775b1ff68b"
                  },
                  {
                    "y": 427,
                    "x": 640,
                    "u": "https://preview.redd.it/3liwqysetv4e1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4d5e1fa31938b2565069f63d80acc7f548380252"
                  }
                ],
                "s": {
                  "y": 427,
                  "x": 640,
                  "u": "https://preview.redd.it/3liwqysetv4e1.png?width=640&amp;format=png&amp;auto=webp&amp;s=3640abecd874d96c89b0c68e7bdcd1f80cf9f76b"
                },
                "id": "3liwqysetv4e1"
              },
              "xi4zklxftv4e1": {
                "status": "valid",
                "e": "Image",
                "m": "image/png",
                "p": [
                  {
                    "y": 43,
                    "x": 108,
                    "u": "https://preview.redd.it/xi4zklxftv4e1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2fd0bf67dec97e7dcf47bc634f3066bca9d9f763"
                  },
                  {
                    "y": 86,
                    "x": 216,
                    "u": "https://preview.redd.it/xi4zklxftv4e1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=cb76e0810bc34b964537a1d99aba3cf7a50e5fd5"
                  },
                  {
                    "y": 128,
                    "x": 320,
                    "u": "https://preview.redd.it/xi4zklxftv4e1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f0be68e213c0161add8f2cb0bc769c297ab4e04b"
                  },
                  {
                    "y": 256,
                    "x": 640,
                    "u": "https://preview.redd.it/xi4zklxftv4e1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c4bba2932d9d6f076d312153ffc0390a6c9f62b4"
                  },
                  {
                    "y": 384,
                    "x": 960,
                    "u": "https://preview.redd.it/xi4zklxftv4e1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=18fbb9a8a7871d5679551409ddf560eda7d656a5"
                  }
                ],
                "s": {
                  "y": 400,
                  "x": 1000,
                  "u": "https://preview.redd.it/xi4zklxftv4e1.png?width=1000&amp;format=png&amp;auto=webp&amp;s=15cdba4212771030abe1e8b6ba8e4a36f44aeefd"
                },
                "id": "xi4zklxftv4e1"
              }
            },
            "name": "t3_1h6ojwr",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 1.0,
            "author_flair_background_color": null,
            "ups": 87,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_5wukhd4",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Resources",
            "can_mod_post": false,
            "score": 87,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://b.thumbs.redditmedia.com/TBHHloA-RsLmFDKq3GJCP3R2Sm2CMcLdE2_zHcm80Js.jpg",
            "edited": 1733340585.0,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "subreddit_type": "public",
            "created": 1733339959.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;! I added 2x faster vision finetuning support in &lt;a href=\"https://github.com/unslothai/unsloth\"&gt;Unsloth&lt;/a&gt;, but some people complained about 4bit quants not performing well. I did an investigation, and it looks like quantizing all layers to 4bit will sometimes break your model! I uploaded mixed 4bit and 16bit weights which aim to recover the accuracy fully.&lt;/p&gt;\n\n&lt;p&gt;For example using Qwen2-VL-2B Instruct, and given an image below:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/3liwqysetv4e1.png?width=640&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3640abecd874d96c89b0c68e7bdcd1f80cf9f76b\"&gt;https://preview.redd.it/3liwqysetv4e1.png?width=640&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3640abecd874d96c89b0c68e7bdcd1f80cf9f76b&lt;/a&gt;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Quantization&lt;/th&gt;\n&lt;th align=\"left\"&gt;Description&lt;/th&gt;\n&lt;th align=\"left\"&gt;Size&lt;/th&gt;\n&lt;th align=\"left\"&gt;Result&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;16bit&lt;/td&gt;\n&lt;td align=\"left\"&gt;The image shows a train traveling on tracks.&lt;/td&gt;\n&lt;td align=\"left\"&gt;4.11GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;\u2705&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Default 4bit all layers&lt;/td&gt;\n&lt;td align=\"left\"&gt;The image depicts a vibrant and colorful scene of a coastal area.&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.36GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;\u274c Definitely wrong&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Unsloth quant&lt;/td&gt;\n&lt;td align=\"left\"&gt;The image shows a train traveling on tracks.&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.81GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;\u2705&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;We see 4bit on all layers breaks Qwen2-VL-2B Instruct. So the trick is to carefully select only some layers to quantize and leave 10% or so in full precision! The main issue is some layers have large outliers, and so we have to inspect both the activation errors (like AWQ) and also weight quantization errors (like HQQ / bitsandbytes). For example if you look at Llama 3.2 11B Vision Instruct&amp;#39;s error analysis below:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/xi4zklxftv4e1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=15cdba4212771030abe1e8b6ba8e4a36f44aeefd\"&gt;https://preview.redd.it/xi4zklxftv4e1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=15cdba4212771030abe1e8b6ba8e4a36f44aeefd&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;We see that:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;There is a large spike in activation error in a MLP layer.&lt;/li&gt;\n&lt;li&gt;There are large repeating spikes in weight quantization errors, and these correspond to the the Cross Attention layers.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I uploaded all dynamic Unsloth quants below. I also attached free Colab Notebooks to finetune / do inference on vision models with Unsloth up to &lt;strong&gt;2x faster and use up to 50% less VRAM&lt;/strong&gt;!&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Model&lt;/th&gt;\n&lt;th align=\"left\"&gt;Model Page&lt;/th&gt;\n&lt;th align=\"left\"&gt;Colab Notebook&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama 3.2 11B Vision Instruct&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/unsloth/Llama-3.2-11B-Vision-Instruct-unsloth-bnb-4bit\"&gt;Dynamic quant&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://colab.research.google.com/drive/1j0N4XTY1zXXy7mPAhOC1_gMYZ2F2EBlk?usp=sharing\"&gt;Colab Notebook&lt;/a&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama 3.2 11B Vision Base&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/unsloth/Llama-3.2-11B-Vision-unsloth-bnb-4bit\"&gt;Dynamic quant&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;Change model name in &lt;a href=\"https://colab.research.google.com/drive/1j0N4XTY1zXXy7mPAhOC1_gMYZ2F2EBlk?usp=sharing\"&gt;Llama 11B Instruct Notebook&lt;/a&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen2 VL 2B Instruct&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/unsloth/Qwen2-VL-2B-Instruct-unsloth-bnb-4bit\"&gt;Dynamic quant&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;Change model name in &lt;a href=\"https://colab.research.google.com/drive/1whHb54GNZMrNxIsi2wm2EY_-Pvo2QyKh?usp=sharing\"&gt;Qwen 7B Instruct Notebook&lt;/a&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen2 VL 7B Instruct&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/unsloth/Qwen2-VL-7B-Instruct-unsloth-bnb-4bit\"&gt;Dynamic quant&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://colab.research.google.com/drive/1whHb54GNZMrNxIsi2wm2EY_-Pvo2QyKh?usp=sharing\"&gt;Colab Notebook&lt;/a&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Pixtral 12B Instruct&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/unsloth/Pixtral-12B-2409-unsloth-bnb-4bit\"&gt;Dynamic quant&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://colab.research.google.com/drive/1K9ZrdwvZRE96qGkCq_e88FgV3MLnymQq?usp=sharing\"&gt;Colab Notebook&lt;/a&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;QwQ 32B Preview&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/unsloth/QwQ-32B-Preview-unsloth-bnb-4bit\"&gt;Dynamic quant&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;Change model name in &lt;a href=\"https://colab.research.google.com/drive/1qN1CEalC70EO1wGKhNxs1go1W9So61R5?usp=sharing\"&gt;Qwen 2.5 Coder Notebook&lt;/a&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;I added more experiments and details in the blog post here: &lt;a href=\"https://unsloth.ai/blog/dynamic-4bit\"&gt;https://unsloth.ai/blog/dynamic-4bit&lt;/a&gt; . Also there are some bugs / issues which I fixed as well in Unsloth, so please update it!&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Llama.cpp GGUF changed from &lt;code&gt;make&lt;/code&gt; to &lt;code&gt;cmake&lt;/code&gt; breaking saving&lt;/li&gt;\n&lt;li&gt;Finetuning then merging to 16bit broke - fixed this now!&lt;/li&gt;\n&lt;li&gt;V100s and older GPUs broke for finetuning - fixed as well!&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Please update Unsloth via &lt;code&gt;pip install --upgrade --no-cache-dir --no-deps unsloth unsloth_zoo&lt;/code&gt;! I also put free Colabs and Kaggle notebooks to finetune Llama, Mistral, Gemma, Phi, Qwen and more on the Github here: &lt;a href=\"https://github.com/unslothai/unsloth\"&gt;https://github.com/unslothai/unsloth&lt;/a&gt; and all model uploads are here: &lt;a href=\"https://huggingface.co/unsloth\"&gt;https://huggingface.co/unsloth&lt;/a&gt; . Thanks a lot and have a great day!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": true,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/GToYANeKQHKhFdKJjLK03Emv1ylZ0l8jeD1iuQJ8-dE.jpg?auto=webp&amp;s=fb46a23aaa0ed1c5044eaea486ff79352cce2675",
                    "width": 1584,
                    "height": 834
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/GToYANeKQHKhFdKJjLK03Emv1ylZ0l8jeD1iuQJ8-dE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6481fbac644d8a96c2918c63e805d1c62e24cbe5",
                      "width": 108,
                      "height": 56
                    },
                    {
                      "url": "https://external-preview.redd.it/GToYANeKQHKhFdKJjLK03Emv1ylZ0l8jeD1iuQJ8-dE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=941b00cf4a68a70df266160fe06769bc2a817a41",
                      "width": 216,
                      "height": 113
                    },
                    {
                      "url": "https://external-preview.redd.it/GToYANeKQHKhFdKJjLK03Emv1ylZ0l8jeD1iuQJ8-dE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e794c7cbf042b8d8e6fdd8f8c239e0f5cb398261",
                      "width": 320,
                      "height": 168
                    },
                    {
                      "url": "https://external-preview.redd.it/GToYANeKQHKhFdKJjLK03Emv1ylZ0l8jeD1iuQJ8-dE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=57fbf9c89972d5c31e3bd2d3354696be4e8d5b9d",
                      "width": 640,
                      "height": 336
                    },
                    {
                      "url": "https://external-preview.redd.it/GToYANeKQHKhFdKJjLK03Emv1ylZ0l8jeD1iuQJ8-dE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=557f9a403410be41c1438b6d2b1a2acd9d507da4",
                      "width": 960,
                      "height": 505
                    },
                    {
                      "url": "https://external-preview.redd.it/GToYANeKQHKhFdKJjLK03Emv1ylZ0l8jeD1iuQJ8-dE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=989ea96f774aa62c199da9564be3b7b646db1494",
                      "width": 1080,
                      "height": 568
                    }
                  ],
                  "variants": {},
                  "id": "oUAe34zUCLxMUIpYtOvOz6aYou2CnbtJjhJZ0bwJ6Jg"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "mod_note": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "num_reports": null,
            "removal_reason": null,
            "link_flair_background_color": "#ccac2b",
            "id": "1h6ojwr",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "danielhanchen",
            "discussion_type": null,
            "num_comments": 43,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/",
            "subreddit_subscribers": 252381,
            "created_utc": 1733339959.0,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "95r93m5fl98612cbd1ad040091006430602872d1c73c019305",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "95r93m5fl98612cbd1ad040091006430602872d1c73c019305",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "d40ca12a-0e73-11ee-8563-f216e082168e",
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "95r93m5fl98612cbd1ad040091006430602872d1c73c019305",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "95r93m5fl98612cbd1ad040091006430602872d1c73c019305",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": "0e2e7958-9549-11ee-a999-027a9c984b05",
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "95r93m5fl98612cbd1ad040091006430602872d1c73c019305",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": {
                                                      "kind": "Listing",
                                                      "data": {
                                                        "after": null,
                                                        "dist": null,
                                                        "modhash": "95r93m5fl98612cbd1ad040091006430602872d1c73c019305",
                                                        "geo_filter": "",
                                                        "children": [
                                                          {
                                                            "kind": "t1",
                                                            "data": {
                                                              "subreddit_id": "t5_81eyvm",
                                                              "approved_at_utc": null,
                                                              "author_is_blocked": false,
                                                              "comment_type": null,
                                                              "awarders": [],
                                                              "mod_reason_by": null,
                                                              "banned_by": null,
                                                              "author_flair_type": "text",
                                                              "total_awards_received": 0,
                                                              "subreddit": "LocalLLaMA",
                                                              "author_flair_template_id": "0e2e7958-9549-11ee-a999-027a9c984b05",
                                                              "distinguished": null,
                                                              "likes": null,
                                                              "replies": "",
                                                              "user_reports": [],
                                                              "saved": false,
                                                              "id": "m0gfsy5",
                                                              "banned_at_utc": null,
                                                              "mod_reason_title": null,
                                                              "gilded": 0,
                                                              "archived": false,
                                                              "collapsed_reason_code": null,
                                                              "no_follow": true,
                                                              "author": "noneabove1182",
                                                              "can_mod_post": false,
                                                              "send_replies": true,
                                                              "parent_id": "t1_m0g8whz",
                                                              "score": 1,
                                                              "author_fullname": "t2_7quep",
                                                              "approved_by": null,
                                                              "mod_note": null,
                                                              "all_awardings": [],
                                                              "body": "Definitely possible, though they do regularly leave weights at 8/6 bits, the one thing it doesn't do though is dynamically choose them, it's more predetermined layers if memory serves\n\n\nSo yeah, GGUF could stand to dynamically quant as well, its current strategy is surprisingly good and robust, but there's room to grow",
                                                              "edited": false,
                                                              "gildings": {},
                                                              "downs": 0,
                                                              "author_flair_css_class": null,
                                                              "name": "t1_m0gfsy5",
                                                              "is_submitter": false,
                                                              "collapsed": false,
                                                              "author_flair_richtext": [],
                                                              "author_patreon_flair": false,
                                                              "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Definitely possible, though they do regularly leave weights at 8/6 bits, the one thing it doesn&amp;#39;t do though is dynamically choose them, it&amp;#39;s more predetermined layers if memory serves&lt;/p&gt;\n\n&lt;p&gt;So yeah, GGUF could stand to dynamically quant as well, its current strategy is surprisingly good and robust, but there&amp;#39;s room to grow&lt;/p&gt;\n&lt;/div&gt;",
                                                              "removal_reason": null,
                                                              "collapsed_reason": null,
                                                              "link_id": "t3_1h6ojwr",
                                                              "associated_award": null,
                                                              "stickied": false,
                                                              "author_premium": false,
                                                              "can_gild": false,
                                                              "top_awarded_type": null,
                                                              "unrepliable_reason": null,
                                                              "author_flair_text_color": "light",
                                                              "score_hidden": false,
                                                              "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/m0gfsy5/",
                                                              "subreddit_type": "public",
                                                              "locked": false,
                                                              "report_reasons": null,
                                                              "created": 1733356455.0,
                                                              "author_flair_text": "Bartowski",
                                                              "treatment_tags": [],
                                                              "created_utc": 1733356455.0,
                                                              "subreddit_name_prefixed": "r/LocalLLaMA",
                                                              "controversiality": 0,
                                                              "depth": 5,
                                                              "author_flair_background_color": "#889bdb",
                                                              "collapsed_because_crowd_control": null,
                                                              "mod_reports": [],
                                                              "num_reports": null,
                                                              "ups": 1
                                                            }
                                                          }
                                                        ],
                                                        "before": null
                                                      }
                                                    },
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "m0g8whz",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "danielhanchen",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_m0fwmtj",
                                                    "score": 2,
                                                    "author_fullname": "t2_5wukhd4",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "Actually I remember the investigation of Qwen 2.5 Coder lower quants don't do well - it's possible some GGUF formats should actually leave some layers in 8bits / 16bits",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_m0g8whz",
                                                    "is_submitter": true,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Actually I remember the investigation of Qwen 2.5 Coder lower quants don&amp;#39;t do well - it&amp;#39;s possible some GGUF formats should actually leave some layers in 8bits / 16bits&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1h6ojwr",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/m0g8whz/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1733354035.0,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1733354035.0,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 2
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "m0fwmtj",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": false,
                                          "author": "noneabove1182",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_m0fpt5f",
                                          "score": 5,
                                          "author_fullname": "t2_7quep",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "This is something different from GGUF, this is more similar to BNB compression but with intelligence. GGUF is already quantizing intelligently (but you can't use those models for finetuning etc)",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_m0fwmtj",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This is something different from GGUF, this is more similar to BNB compression but with intelligence. GGUF is already quantizing intelligently (but you can&amp;#39;t use those models for finetuning etc)&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1h6ojwr",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": "light",
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/m0fwmtj/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1733350047.0,
                                          "author_flair_text": "Bartowski",
                                          "treatment_tags": [],
                                          "created_utc": 1733350047.0,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": "#889bdb",
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 5
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "m0fpt5f",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Shir_man",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_m0fkvxu",
                                "score": 3,
                                "author_fullname": "t2_366vr",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "yay! thank you!\n\n  \nUPD. Wait, no GGUF yet?",
                                "edited": 1733349102.0,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_m0fpt5f",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;yay! thank you!&lt;/p&gt;\n\n&lt;p&gt;UPD. Wait, no GGUF yet?&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1h6ojwr",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/m0fpt5f/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1733348003.0,
                                "author_flair_text": "llama.cpp",
                                "treatment_tags": [],
                                "created_utc": 1733348003.0,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#bbbdbf",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 3
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "m0fkvxu",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "yoracale",
                      "can_mod_post": false,
                      "created_utc": 1733346585.0,
                      "send_replies": true,
                      "parent_id": "t1_m0fin86",
                      "score": 8,
                      "author_fullname": "t2_1162lx9rgr",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Yes this also applies to text models as well. We will release a separate blog post for that along with model uploads for text based.\n\nWe do not make dynamic quants on the fly with unsloth so you will need to download them directly from hugging face.\n\nBtw we uploaded [QwQ-32B-Preview](https://huggingface.co/unsloth/QwQ-32B-Preview-unsloth-bnb-4bit) for now as the first text based model using the dynamic quants method.",
                      "edited": 1733347013.0,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_m0fkvxu",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes this also applies to text models as well. We will release a separate blog post for that along with model uploads for text based.&lt;/p&gt;\n\n&lt;p&gt;We do not make dynamic quants on the fly with unsloth so you will need to download them directly from hugging face.&lt;/p&gt;\n\n&lt;p&gt;Btw we uploaded &lt;a href=\"https://huggingface.co/unsloth/QwQ-32B-Preview-unsloth-bnb-4bit\"&gt;QwQ-32B-Preview&lt;/a&gt; for now as the first text based model using the dynamic quants method.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1h6ojwr",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "light",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/m0fkvxu/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1733346585.0,
                      "author_flair_text": "Llama 2",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "#ab96c2",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 8
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "m0fin86",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Igoory",
            "can_mod_post": false,
            "created_utc": 1733345929.0,
            "send_replies": true,
            "parent_id": "t3_1h6ojwr",
            "score": 6,
            "author_fullname": "t2_40n0ldkg",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "This is very interesting, so I guess this also improves plain language models? And if I use fp16 weights, will unsloth automatically make a dynamic quant or do I need to use the quants uploaded by you guys? If it's the later, it would be nice if there was a script available to make these quants so anyone could make them too!",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_m0fin86",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This is very interesting, so I guess this also improves plain language models? And if I use fp16 weights, will unsloth automatically make a dynamic quant or do I need to use the quants uploaded by you guys? If it&amp;#39;s the later, it would be nice if there was a script available to make these quants so anyone could make them too!&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/m0fin86/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1733345929.0,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1h6ojwr",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 6
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "2b12e2b8-fdc0-11ee-9a03-6e2f48afd456",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "95r93m5fl98612cbd1ad040091006430602872d1c73c019305",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "m0g8pwu",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "danielhanchen",
                      "can_mod_post": false,
                      "created_utc": 1733353972.0,
                      "send_replies": true,
                      "parent_id": "t1_m0fdvij",
                      "score": 2,
                      "author_fullname": "t2_5wukhd4",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Oh :( Hmmm multiple people have asked for this hmmm",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_m0g8pwu",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Oh :( Hmmm multiple people have asked for this hmmm&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1h6ojwr",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/m0g8pwu/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1733353972.0,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "m0fdvij",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "kryptkpr",
            "can_mod_post": false,
            "created_utc": 1733344527.0,
            "send_replies": true,
            "parent_id": "t3_1h6ojwr",
            "score": 4,
            "author_fullname": "t2_30i1a",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Great work! Is there any OpenAI vision compatible API server that can support these hybrids? I am having a lot of trouble locally running VLMs and getting them to work as drop-in replacements for Omni.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_m0fdvij",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Great work! Is there any OpenAI vision compatible API server that can support these hybrids? I am having a lot of trouble locally running VLMs and getting them to work as drop-in replacements for Omni.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/m0fdvij/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1733344527.0,
            "author_flair_text": "Llama 3",
            "treatment_tags": [],
            "link_id": "t3_1h6ojwr",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#c7b594",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "95r93m5fl98612cbd1ad040091006430602872d1c73c019305",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "m0g69sh",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "danielhanchen",
                      "can_mod_post": false,
                      "created_utc": 1733353136.0,
                      "send_replies": true,
                      "parent_id": "t1_m0fmuw6",
                      "score": 2,
                      "author_fullname": "t2_5wukhd4",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Yep empirically and anecdotal evidence does look like this was the case!",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_m0g69sh",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yep empirically and anecdotal evidence does look like this was the case!&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1h6ojwr",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/m0g69sh/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1733353136.0,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "m0fmuw6",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "lordpuddingcup",
            "can_mod_post": false,
            "created_utc": 1733347150.0,
            "send_replies": true,
            "parent_id": "t3_1h6ojwr",
            "score": 3,
            "author_fullname": "t2_vc4z2",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Ya we\u2019ve seen this with gguf quants on flux some layers just need high precision while others can go MUCH lower",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_m0fmuw6",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ya we\u2019ve seen this with gguf quants on flux some layers just need high precision while others can go MUCH lower&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/m0fmuw6/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1733347150.0,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1h6ojwr",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "95r93m5fl98612cbd1ad040091006430602872d1c73c019305",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "m0g5y2a",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "danielhanchen",
                      "can_mod_post": false,
                      "created_utc": 1733353028.0,
                      "send_replies": true,
                      "parent_id": "t1_m0fv01x",
                      "score": 1,
                      "author_fullname": "t2_5wukhd4",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks!!",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_m0g5y2a",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks!!&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1h6ojwr",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/m0g5y2a/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1733353028.0,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "m0fv01x",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Educational_Rent1059",
            "can_mod_post": false,
            "created_utc": 1733349546.0,
            "send_replies": true,
            "parent_id": "t3_1h6ojwr",
            "score": 3,
            "author_fullname": "t2_ac1d5rhvu",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Awesome work as always by you guys!!! Amazing",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_m0fv01x",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Awesome work as always by you guys!!! Amazing&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/m0fv01x/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1733349546.0,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1h6ojwr",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "95r93m5fl98612cbd1ad040091006430602872d1c73c019305",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "m0g5ut7",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "danielhanchen",
                      "can_mod_post": false,
                      "created_utc": 1733352998.0,
                      "send_replies": true,
                      "parent_id": "t1_m0fvqdi",
                      "score": 1,
                      "author_fullname": "t2_5wukhd4",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks and appreciate it :)",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_m0g5ut7",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks and appreciate it :)&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1h6ojwr",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/m0g5ut7/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1733352998.0,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "m0fvqdi",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Mobile_Tart_1016",
            "can_mod_post": false,
            "created_utc": 1733349768.0,
            "send_replies": true,
            "parent_id": "t3_1h6ojwr",
            "score": 3,
            "author_fullname": "t2_9pnhbbia3",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I really like that people start to debug models like you did.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_m0fvqdi",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I really like that people start to debug models like you did.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/m0fvqdi/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1733349768.0,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1h6ojwr",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "95r93m5fl98612cbd1ad040091006430602872d1c73c019305",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "95r93m5fl98612cbd1ad040091006430602872d1c73c019305",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "95r93m5fl98612cbd1ad040091006430602872d1c73c019305",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "m0f8u46",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "danielhanchen",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_m0f7dft",
                                          "score": 2,
                                          "author_fullname": "t2_5wukhd4",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Fantastic question!! You could in theory take the answers, and train on them, but I would suggest getting the entire chain of thought",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_m0f8u46",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Fantastic question!! You could in theory take the answers, and train on them, but I would suggest getting the entire chain of thought&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1h6ojwr",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/m0f8u46/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1733343004.0,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1733343004.0,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 2
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "m0f7dft",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Few_Painter_5588",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_m0f22vb",
                                "score": 2,
                                "author_fullname": "t2_uvgafqnfy",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Thanks for the detailed answers! Would you recommend training QwQ on answers, or should we train it on the \"thoughts\" that lead to the answer?",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_m0f7dft",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks for the detailed answers! Would you recommend training QwQ on answers, or should we train it on the &amp;quot;thoughts&amp;quot; that lead to the answer?&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1h6ojwr",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/m0f7dft/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1733342570.0,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1733342570.0,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "95r93m5fl98612cbd1ad040091006430602872d1c73c019305",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "m0fc530",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "danielhanchen",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_m0fb6aq",
                                          "score": 2,
                                          "author_fullname": "t2_5wukhd4",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Oh I would recommend vLLM - we have saving options after finetuning for vLLM. Unsloth single batch 4bit is much faster than vLLM, but batched is similar.\n\nI'm unsure if the dynamic quants work in vLLM - but 4bit QwQ should generally be OK",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_m0fc530",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Oh I would recommend vLLM - we have saving options after finetuning for vLLM. Unsloth single batch 4bit is much faster than vLLM, but batched is similar.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m unsure if the dynamic quants work in vLLM - but 4bit QwQ should generally be OK&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1h6ojwr",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/m0fc530/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1733343998.0,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1733343998.0,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 2
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "m0fb6aq",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "ResidentPositive4122",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_m0f22vb",
                                "score": 1,
                                "author_fullname": "t2_10nxrjjgay",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "So what would be the best way to quantize QwQ for running inference with vLLM?\n\nDoes unsloth support batched inference? Is it comparable in tok/s throughput w/ vLLM?",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_m0fb6aq",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;So what would be the best way to quantize QwQ for running inference with vLLM?&lt;/p&gt;\n\n&lt;p&gt;Does unsloth support batched inference? Is it comparable in tok/s throughput w/ vLLM?&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1h6ojwr",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/m0fb6aq/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1733343707.0,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1733343707.0,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "m0f22vb",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "danielhanchen",
                      "can_mod_post": false,
                      "created_utc": 1733340983.0,
                      "send_replies": true,
                      "parent_id": "t1_m0ezpzs",
                      "score": 6,
                      "author_fullname": "t2_5wukhd4",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Oh I also added a plot for QwQ dynamic quants - [https://huggingface.co/unsloth/QwQ-32B-Preview-unsloth-bnb-4bit](https://huggingface.co/unsloth/QwQ-32B-Preview-unsloth-bnb-4bit)\n\nhttps://preview.redd.it/glwftgxjwv4e1.png?width=1000&amp;format=png&amp;auto=webp&amp;s=e71332751d2e2623e94cd8b9f44e7c26f410dad8\n\nQwQ does have some large spikes for activations for 4bit, and weight quantization errors have a few spikes.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_m0f22vb",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Oh I also added a plot for QwQ dynamic quants - &lt;a href=\"https://huggingface.co/unsloth/QwQ-32B-Preview-unsloth-bnb-4bit\"&gt;https://huggingface.co/unsloth/QwQ-32B-Preview-unsloth-bnb-4bit&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/glwftgxjwv4e1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e71332751d2e2623e94cd8b9f44e7c26f410dad8\"&gt;https://preview.redd.it/glwftgxjwv4e1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e71332751d2e2623e94cd8b9f44e7c26f410dad8&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;QwQ does have some large spikes for activations for 4bit, and weight quantization errors have a few spikes.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1h6ojwr",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/m0f22vb/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1733340983.0,
                      "media_metadata": {
                        "glwftgxjwv4e1": {
                          "status": "valid",
                          "e": "Image",
                          "m": "image/png",
                          "p": [
                            {
                              "y": 43,
                              "x": 108,
                              "u": "https://preview.redd.it/glwftgxjwv4e1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3b0013e6dfe05384f8c79b69d21e84aca0d101a8"
                            },
                            {
                              "y": 86,
                              "x": 216,
                              "u": "https://preview.redd.it/glwftgxjwv4e1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=73f8739d7de44ae8b041b7f73d73c6a4c027fea3"
                            },
                            {
                              "y": 128,
                              "x": 320,
                              "u": "https://preview.redd.it/glwftgxjwv4e1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1fc19bc4c6ef12bad668f06e180393a6a60b825b"
                            },
                            {
                              "y": 256,
                              "x": 640,
                              "u": "https://preview.redd.it/glwftgxjwv4e1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=403423d5b8d3dbe65fea4c0485f60ee3e4cc4725"
                            },
                            {
                              "y": 384,
                              "x": 960,
                              "u": "https://preview.redd.it/glwftgxjwv4e1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1d5d49757ad2d3ba6c99fe85ee37669a414169b0"
                            }
                          ],
                          "s": {
                            "y": 400,
                            "x": 1000,
                            "u": "https://preview.redd.it/glwftgxjwv4e1.png?width=1000&amp;format=png&amp;auto=webp&amp;s=e71332751d2e2623e94cd8b9f44e7c26f410dad8"
                          },
                          "id": "glwftgxjwv4e1"
                        }
                      },
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 6
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "m0f0xwd",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "danielhanchen",
                      "can_mod_post": false,
                      "created_utc": 1733340639.0,
                      "send_replies": true,
                      "parent_id": "t1_m0ezpzs",
                      "score": 7,
                      "author_fullname": "t2_5wukhd4",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Oh yes QwQ is in fact supported :) You can just load it up as usual - Use the Qwen 2.5 Coder notebook here: https://colab.research.google.com/drive/1qN1CEalC70EO1wGKhNxs1go1W9So61R5?usp=sharing and just change them model name to `unsloth/QwQ-32B-Preview`",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_m0f0xwd",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Oh yes QwQ is in fact supported :) You can just load it up as usual - Use the Qwen 2.5 Coder notebook here: &lt;a href=\"https://colab.research.google.com/drive/1qN1CEalC70EO1wGKhNxs1go1W9So61R5?usp=sharing\"&gt;https://colab.research.google.com/drive/1qN1CEalC70EO1wGKhNxs1go1W9So61R5?usp=sharing&lt;/a&gt; and just change them model name to &lt;code&gt;unsloth/QwQ-32B-Preview&lt;/code&gt;&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1h6ojwr",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/m0f0xwd/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1733340639.0,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 7
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "m0f12f6",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "danielhanchen",
                      "can_mod_post": false,
                      "created_utc": 1733340677.0,
                      "send_replies": true,
                      "parent_id": "t1_m0ezpzs",
                      "score": 6,
                      "author_fullname": "t2_5wukhd4",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Just don't forget to update Unsloth if on a local machine via `pip install --upgrade --no-cache-dir --no-deps unsloth unsloth_zoo`! Colabs and Kaggles just need to refresh the notebook",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_m0f12f6",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Just don&amp;#39;t forget to update Unsloth if on a local machine via &lt;code&gt;pip install --upgrade --no-cache-dir --no-deps unsloth unsloth_zoo&lt;/code&gt;! Colabs and Kaggles just need to refresh the notebook&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1h6ojwr",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/m0f12f6/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1733340677.0,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 6
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "m0ezpzs",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Few_Painter_5588",
            "can_mod_post": false,
            "created_utc": 1733340274.0,
            "send_replies": true,
            "parent_id": "t3_1h6ojwr",
            "score": 6,
            "author_fullname": "t2_uvgafqnfy",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "QwQ is supported in Unsloth? How does one go about finetuning it?\n\nRegardless, awesome work and keep it up! Y'all are real ones \ud83d\udd25",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_m0ezpzs",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;QwQ is supported in Unsloth? How does one go about finetuning it?&lt;/p&gt;\n\n&lt;p&gt;Regardless, awesome work and keep it up! Y&amp;#39;all are real ones \ud83d\udd25&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/m0ezpzs/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1733340274.0,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1h6ojwr",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 6
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "95r93m5fl98612cbd1ad040091006430602872d1c73c019305",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "d40ca12a-0e73-11ee-8563-f216e082168e",
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "m0f5vtn",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "yoracale",
                      "can_mod_post": false,
                      "created_utc": 1733342122.0,
                      "send_replies": true,
                      "parent_id": "t1_m0f0hwh",
                      "score": 1,
                      "author_fullname": "t2_1162lx9rgr",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Yay let us know how it goes! :D",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_m0f5vtn",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yay let us know how it goes! :D&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1h6ojwr",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "light",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/m0f5vtn/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1733342122.0,
                      "author_flair_text": "Llama 2",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "#ab96c2",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "m0f0hwh",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "____vladrad",
            "can_mod_post": false,
            "created_utc": 1733340507.0,
            "send_replies": true,
            "parent_id": "t3_1h6ojwr",
            "score": 2,
            "author_fullname": "t2_u6i8a0ay",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Nice read thank you for sharing, I\u2019m working on vision starting tonight so I\u2019m excited to get going",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_m0f0hwh",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Nice read thank you for sharing, I\u2019m working on vision starting tonight so I\u2019m excited to get going&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/m0f0hwh/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1733340507.0,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1h6ojwr",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "95r93m5fl98612cbd1ad040091006430602872d1c73c019305",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "m0f1vmj",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "danielhanchen",
                      "can_mod_post": false,
                      "created_utc": 1733340922.0,
                      "send_replies": true,
                      "parent_id": "t1_m0f18qj",
                      "score": 1,
                      "author_fullname": "t2_5wukhd4",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks!!",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_m0f1vmj",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks!!&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1h6ojwr",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/m0f1vmj/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1733340922.0,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "m0f18qj",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Round_Document6821",
            "can_mod_post": false,
            "created_utc": 1733340729.0,
            "send_replies": true,
            "parent_id": "t3_1h6ojwr",
            "score": 2,
            "author_fullname": "t2_a7joxgocv",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Cool project as always guys!",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_m0f18qj",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Cool project as always guys!&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/m0f18qj/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1733340729.0,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1h6ojwr",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "95r93m5fl98612cbd1ad040091006430602872d1c73c019305",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "m0g64uc",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "danielhanchen",
                      "can_mod_post": false,
                      "created_utc": 1733353090.0,
                      "send_replies": true,
                      "parent_id": "t1_m0ftqp3",
                      "score": 2,
                      "author_fullname": "t2_5wukhd4",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Oh yes you can use AWQ, but the trick we do is we don't need to find some scaling transformation - we simply just let some parameters literally stay in FP16, and the rest in INT4",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_m0g64uc",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Oh yes you can use AWQ, but the trick we do is we don&amp;#39;t need to find some scaling transformation - we simply just let some parameters literally stay in FP16, and the rest in INT4&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1h6ojwr",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/m0g64uc/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1733353090.0,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "m0ftqp3",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "everydayissame",
            "can_mod_post": false,
            "created_utc": 1733349167.0,
            "send_replies": true,
            "parent_id": "t3_1h6ojwr",
            "score": 2,
            "author_fullname": "t2_5idsyzlg",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "How about AWQ 4bit quant? I am curious how it compares.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_m0ftqp3",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;How about AWQ 4bit quant? I am curious how it compares.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/m0ftqp3/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1733349167.0,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1h6ojwr",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "95r93m5fl98612cbd1ad040091006430602872d1c73c019305",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "m0g5xj2",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "danielhanchen",
                      "can_mod_post": false,
                      "created_utc": 1733353023.0,
                      "send_replies": true,
                      "parent_id": "t1_m0g22vv",
                      "score": 4,
                      "author_fullname": "t2_5wukhd4",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Oh we just wanted to release some versions - as part of all model support in Unsloth, we'll add it in!",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_m0g5xj2",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Oh we just wanted to release some versions - as part of all model support in Unsloth, we&amp;#39;ll add it in!&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1h6ojwr",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/m0g5xj2/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1733353023.0,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 4
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "m0g22vv",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "FullOf_Bad_Ideas",
            "can_mod_post": false,
            "created_utc": 1733351763.0,
            "send_replies": true,
            "parent_id": "t3_1h6ojwr",
            "score": 2,
            "author_fullname": "t2_9s7pmakgx",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Can you please release code needed to perform this manually for models where you didn't upload the quants? I'm planning to finetune Qwen2 VL 72B with QLoRA and I would also like to see how this affects text only llm's I've been using qlora on.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_m0g22vv",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Can you please release code needed to perform this manually for models where you didn&amp;#39;t upload the quants? I&amp;#39;m planning to finetune Qwen2 VL 72B with QLoRA and I would also like to see how this affects text only llm&amp;#39;s I&amp;#39;ve been using qlora on.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/m0g22vv/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1733351763.0,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1h6ojwr",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "95r93m5fl98612cbd1ad040091006430602872d1c73c019305",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "d40ca12a-0e73-11ee-8563-f216e082168e",
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "95r93m5fl98612cbd1ad040091006430602872d1c73c019305",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "95r93m5fl98612cbd1ad040091006430602872d1c73c019305",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "m0g8mtx",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "danielhanchen",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_m0fn42e",
                                          "score": 3,
                                          "author_fullname": "t2_5wukhd4",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Oh yep the vision encoder generally shouldn't be in 4bit, but Llama seems OK with it - Llava based models don't like it (Qwen, Pixtral) etc.\n\nThere are other layers that are non vision parts which cause issues as well - the model config file should have which layers look problematic!",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_m0g8mtx",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Oh yep the vision encoder generally shouldn&amp;#39;t be in 4bit, but Llama seems OK with it - Llava based models don&amp;#39;t like it (Qwen, Pixtral) etc.&lt;/p&gt;\n\n&lt;p&gt;There are other layers that are non vision parts which cause issues as well - the model config file should have which layers look problematic!&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1h6ojwr",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/m0g8mtx/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1733353943.0,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1733353943.0,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 3
                                        }
                                      },
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "95r93m5fl98612cbd1ad040091006430602872d1c73c019305",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": "",
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "m0gdt3b",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "a_beautiful_rhind",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_m0g8vm8",
                                                    "score": 1,
                                                    "author_fullname": "t2_h5utwre7",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "Try to leave out:     \n\n    input_layernorm\n    mlp\n    post_attention_layernorm\n\nWhen I skipped those merging, it spoke more like the vision model than the RP tune.",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_m0gdt3b",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Try to leave out:     &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;input_layernorm\nmlp\npost_attention_layernorm\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;When I skipped those merging, it spoke more like the vision model than the RP tune.&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1h6ojwr",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/m0gdt3b/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1733355754.0,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1733355754.0,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 1
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "m0g8vm8",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "FullOf_Bad_Ideas",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_m0fn42e",
                                          "score": 2,
                                          "author_fullname": "t2_9s7pmakgx",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Fp8 llm-compressor quantized Qwen2-VL-7B has some issues even if I leave the vision tower intact. Vision tower is the most important but it does seem like there might be individual outlier layers too.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_m0g8vm8",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Fp8 llm-compressor quantized Qwen2-VL-7B has some issues even if I leave the vision tower intact. Vision tower is the most important but it does seem like there might be individual outlier layers too.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1h6ojwr",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/m0g8vm8/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1733354027.0,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1733354027.0,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 2
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "m0fn42e",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "a_beautiful_rhind",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_m0fl36o",
                                "score": 1,
                                "author_fullname": "t2_h5utwre7",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Should be a layer class though, right? Like MLP or one of the self attentions? Rather than a particular layer number.\n\nFor instance, text layers in qwen are composed like this:\n\n    \"model.layers.1.input_layernorm.weight\": \"model-00001-of-00005.safetensors\",\n    \"model.layers.1.mlp.down_proj.weight\": \"model-00001-of-00005.safetensors\",\n    \"model.layers.1.mlp.gate_proj.weight\": \"model-00001-of-00005.safetensors\",\n    \"model.layers.1.mlp.up_proj.weight\": \"model-00001-of-00005.safetensors\",\n    \"model.layers.1.post_attention_layernorm.weight\": \"model-00001-of-00005.safetensors\",\n    \"model.layers.1.self_attn.k_proj.bias\": \"model-00001-of-00005.safetensors\",\n    \"model.layers.1.self_attn.k_proj.weight\": \"model-00001-of-00005.safetensors\",\n    \"model.layers.1.self_attn.o_proj.weight\": \"model-00001-of-00005.safetensors\",\n    \"model.layers.1.self_attn.q_proj.bias\": \"model-00001-of-00005.safetensors\",\n    \"model.layers.1.self_attn.q_proj.weight\": \"model-00001-of-00005.safetensors\",\n    \"model.layers.1.self_attn.v_proj.bias\": \"model-00001-of-00005.safetensors\",\n    \"model.layers.1.self_attn.v_proj.weight\": \"model-00001-of-00005.safetensors\",\n\nVisual blocks are labeled and easy to leave alone\n\n    \"visual.blocks.4.attn.proj.bias\": \"model-00001-of-00005.safetensors\",\n    \"visual.blocks.4.attn.proj.weight\": \"model-00001-of-00005.safetensors\",\n    \"visual.blocks.4.attn.qkv.bias\": \"model-00001-of-00005.safetensors\",\n    \"visual.blocks.4.attn.qkv.weight\": \"model-00001-of-00005.safetensors\",\n    \"visual.blocks.4.mlp.fc1.bias\": \"model-00001-of-00005.safetensors\",\n    \"visual.blocks.4.mlp.fc1.weight\": \"model-00001-of-00005.safetensors\",\n    \"visual.blocks.4.mlp.fc2.bias\": \"model-00001-of-00005.safetensors\",\n    \"visual.blocks.4.mlp.fc2.weight\": \"model-00001-of-00005.safetensors\",\n    \"visual.blocks.4.norm1.bias\": \"model-00001-of-00005.safetensors\",\n    \"visual.blocks.4.norm1.weight\": \"model-00001-of-00005.safetensors\",\n    \"visual.blocks.4.norm2.bias\": \"model-00001-of-00005.safetensors\",\n    \"visual.blocks.4.norm2.weight\": \"model-00001-of-00005.safetensors\",",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_m0fn42e",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Should be a layer class though, right? Like MLP or one of the self attentions? Rather than a particular layer number.&lt;/p&gt;\n\n&lt;p&gt;For instance, text layers in qwen are composed like this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;&amp;quot;model.layers.1.input_layernorm.weight&amp;quot;: &amp;quot;model-00001-of-00005.safetensors&amp;quot;,\n&amp;quot;model.layers.1.mlp.down_proj.weight&amp;quot;: &amp;quot;model-00001-of-00005.safetensors&amp;quot;,\n&amp;quot;model.layers.1.mlp.gate_proj.weight&amp;quot;: &amp;quot;model-00001-of-00005.safetensors&amp;quot;,\n&amp;quot;model.layers.1.mlp.up_proj.weight&amp;quot;: &amp;quot;model-00001-of-00005.safetensors&amp;quot;,\n&amp;quot;model.layers.1.post_attention_layernorm.weight&amp;quot;: &amp;quot;model-00001-of-00005.safetensors&amp;quot;,\n&amp;quot;model.layers.1.self_attn.k_proj.bias&amp;quot;: &amp;quot;model-00001-of-00005.safetensors&amp;quot;,\n&amp;quot;model.layers.1.self_attn.k_proj.weight&amp;quot;: &amp;quot;model-00001-of-00005.safetensors&amp;quot;,\n&amp;quot;model.layers.1.self_attn.o_proj.weight&amp;quot;: &amp;quot;model-00001-of-00005.safetensors&amp;quot;,\n&amp;quot;model.layers.1.self_attn.q_proj.bias&amp;quot;: &amp;quot;model-00001-of-00005.safetensors&amp;quot;,\n&amp;quot;model.layers.1.self_attn.q_proj.weight&amp;quot;: &amp;quot;model-00001-of-00005.safetensors&amp;quot;,\n&amp;quot;model.layers.1.self_attn.v_proj.bias&amp;quot;: &amp;quot;model-00001-of-00005.safetensors&amp;quot;,\n&amp;quot;model.layers.1.self_attn.v_proj.weight&amp;quot;: &amp;quot;model-00001-of-00005.safetensors&amp;quot;,\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Visual blocks are labeled and easy to leave alone&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;&amp;quot;visual.blocks.4.attn.proj.bias&amp;quot;: &amp;quot;model-00001-of-00005.safetensors&amp;quot;,\n&amp;quot;visual.blocks.4.attn.proj.weight&amp;quot;: &amp;quot;model-00001-of-00005.safetensors&amp;quot;,\n&amp;quot;visual.blocks.4.attn.qkv.bias&amp;quot;: &amp;quot;model-00001-of-00005.safetensors&amp;quot;,\n&amp;quot;visual.blocks.4.attn.qkv.weight&amp;quot;: &amp;quot;model-00001-of-00005.safetensors&amp;quot;,\n&amp;quot;visual.blocks.4.mlp.fc1.bias&amp;quot;: &amp;quot;model-00001-of-00005.safetensors&amp;quot;,\n&amp;quot;visual.blocks.4.mlp.fc1.weight&amp;quot;: &amp;quot;model-00001-of-00005.safetensors&amp;quot;,\n&amp;quot;visual.blocks.4.mlp.fc2.bias&amp;quot;: &amp;quot;model-00001-of-00005.safetensors&amp;quot;,\n&amp;quot;visual.blocks.4.mlp.fc2.weight&amp;quot;: &amp;quot;model-00001-of-00005.safetensors&amp;quot;,\n&amp;quot;visual.blocks.4.norm1.bias&amp;quot;: &amp;quot;model-00001-of-00005.safetensors&amp;quot;,\n&amp;quot;visual.blocks.4.norm1.weight&amp;quot;: &amp;quot;model-00001-of-00005.safetensors&amp;quot;,\n&amp;quot;visual.blocks.4.norm2.bias&amp;quot;: &amp;quot;model-00001-of-00005.safetensors&amp;quot;,\n&amp;quot;visual.blocks.4.norm2.weight&amp;quot;: &amp;quot;model-00001-of-00005.safetensors&amp;quot;,\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1h6ojwr",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/m0fn42e/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1733347223.0,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1733347223.0,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "m0fl36o",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "yoracale",
                      "can_mod_post": false,
                      "created_utc": 1733346643.0,
                      "send_replies": true,
                      "parent_id": "t1_m0fedhh",
                      "score": 2,
                      "author_fullname": "t2_1162lx9rgr",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Oh it's selectively chosen for each model so every model will have different configurations.\n\nI guess vision models are also more sensitive because of how the results are more differentiable. It's like finetuning a text based LLM vs finetuning diffusion/voice models where the latter you can clearly see stark differences",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_m0fl36o",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Oh it&amp;#39;s selectively chosen for each model so every model will have different configurations.&lt;/p&gt;\n\n&lt;p&gt;I guess vision models are also more sensitive because of how the results are more differentiable. It&amp;#39;s like finetuning a text based LLM vs finetuning diffusion/voice models where the latter you can clearly see stark differences&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1h6ojwr",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "light",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/m0fl36o/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1733346643.0,
                      "author_flair_text": "Llama 2",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "#ab96c2",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "m0fedhh",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "a_beautiful_rhind",
            "can_mod_post": false,
            "created_utc": 1733344677.0,
            "send_replies": true,
            "parent_id": "t3_1h6ojwr",
            "score": 1,
            "author_fullname": "t2_h5utwre7",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Vison models were always more sensitive. For bits and bytes, had to skip the vison tower entirely or it would get *really* broken.\n\nWhich additional layers are you skipping? I probably want to pass them through when merging too. Didn't see it listed on the blog.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_m0fedhh",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Vison models were always more sensitive. For bits and bytes, had to skip the vison tower entirely or it would get &lt;em&gt;really&lt;/em&gt; broken.&lt;/p&gt;\n\n&lt;p&gt;Which additional layers are you skipping? I probably want to pass them through when merging too. Didn&amp;#39;t see it listed on the blog.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/m0fedhh/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1733344677.0,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1h6ojwr",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "m0feqru",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Medium_Chemist_4032",
            "can_mod_post": false,
            "created_utc": 1733344787.0,
            "send_replies": true,
            "parent_id": "t3_1h6ojwr",
            "score": 2,
            "author_fullname": "t2_aunwxc2u",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I feel that is a true breakthrough",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_m0feqru",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I feel that is a true breakthrough&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/m0feqru/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1733344787.0,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1h6ojwr",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "95r93m5fl98612cbd1ad040091006430602872d1c73c019305",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "d40ca12a-0e73-11ee-8563-f216e082168e",
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "m0fkmqj",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "yoracale",
                      "can_mod_post": false,
                      "created_utc": 1733346511.0,
                      "send_replies": true,
                      "parent_id": "t1_m0ffcix",
                      "score": 1,
                      "author_fullname": "t2_1162lx9rgr",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "This applies to text models as well. We will release a separate blog post for that along with model uploads\n\nBtw we uploaded [QwQ-32B-Preview](https://huggingface.co/unsloth/QwQ-32B-Preview-unsloth-bnb-4bit) for now as the first text based model using the dynamic quants method.",
                      "edited": 1733346988.0,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_m0fkmqj",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This applies to text models as well. We will release a separate blog post for that along with model uploads&lt;/p&gt;\n\n&lt;p&gt;Btw we uploaded &lt;a href=\"https://huggingface.co/unsloth/QwQ-32B-Preview-unsloth-bnb-4bit\"&gt;QwQ-32B-Preview&lt;/a&gt; for now as the first text based model using the dynamic quants method.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1h6ojwr",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "light",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/m0fkmqj/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1733346511.0,
                      "author_flair_text": "Llama 2",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "#ab96c2",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "m0ffcix",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Potential_Rise9790",
            "can_mod_post": false,
            "created_utc": 1733344965.0,
            "send_replies": true,
            "parent_id": "t3_1h6ojwr",
            "score": 1,
            "author_fullname": "t2_jojqojtkc",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "oh very interesting. Is this applicable to only vision models or does this apply to the old text models too?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_m0ffcix",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;oh very interesting. Is this applicable to only vision models or does this apply to the old text models too?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/m0ffcix/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1733344965.0,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1h6ojwr",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "95r93m5fl98612cbd1ad040091006430602872d1c73c019305",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "d40ca12a-0e73-11ee-8563-f216e082168e",
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "m0fkijd",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "yoracale",
                      "can_mod_post": false,
                      "created_utc": 1733346477.0,
                      "send_replies": true,
                      "parent_id": "t1_m0ffiej",
                      "score": 3,
                      "author_fullname": "t2_1162lx9rgr",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "It also works for text based models as well but we firstly are showcasing vision models as it's easier to see the difference. Text based models are a little harder to differentiate I guess. We can make a separate blog post for that\n\nBtw we uploaded [QwQ-32B-Preview](https://huggingface.co/unsloth/QwQ-32B-Preview-unsloth-bnb-4bit) for now as the first text based models using the dynamic quants method.",
                      "edited": 1733346964.0,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_m0fkijd",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It also works for text based models as well but we firstly are showcasing vision models as it&amp;#39;s easier to see the difference. Text based models are a little harder to differentiate I guess. We can make a separate blog post for that&lt;/p&gt;\n\n&lt;p&gt;Btw we uploaded &lt;a href=\"https://huggingface.co/unsloth/QwQ-32B-Preview-unsloth-bnb-4bit\"&gt;QwQ-32B-Preview&lt;/a&gt; for now as the first text based models using the dynamic quants method.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1h6ojwr",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "light",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/m0fkijd/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1733346477.0,
                      "author_flair_text": "Llama 2",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "#ab96c2",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "m0ffiej",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "LosEagle",
            "can_mod_post": false,
            "created_utc": 1733345013.0,
            "send_replies": true,
            "parent_id": "t3_1h6ojwr",
            "score": 1,
            "author_fullname": "t2_iyqcl",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Ah, we are talking about vision models here. From the title I feared this is a more general observation as with my single gpu, quants is all I got.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_m0ffiej",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ah, we are talking about vision models here. From the title I feared this is a more general observation as with my single gpu, quants is all I got.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1h6ojwr/quantizing_to_4bits_can_break_models_dynamic/m0ffiej/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1733345013.0,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1h6ojwr",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]
